# æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼šè®© AI æ¨¡å‹æ›´é«˜æ•ˆçš„ç§˜è¯€

## å¼•è¨€

åœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰è¿…çŒ›å‘å±•çš„ä»Šå¤©ï¼Œå¤§å‹æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚ GPT ç³»åˆ—ã€BERT ç­‰å·²æˆä¸ºä¸»æµã€‚è¿™äº›æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸå±•ç°å‡ºæƒŠäººçš„æ€§èƒ½ï¼Œä½†éšä¹‹è€Œæ¥çš„æ˜¯åºå¤§çš„å‚æ•°é‡å’Œè®¡ç®—èµ„æºéœ€æ±‚ã€‚ä¾‹å¦‚ï¼ŒGPT-3 æ¨¡å‹çš„å‚æ•°è§„æ¨¡é«˜è¾¾ 1750 äº¿ï¼Œè¿™ä½¿å¾—å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚æ‰‹æœºã€æ™ºèƒ½å®¶å±…ï¼‰ä¸Šçš„éƒ¨ç½²å˜å¾—å¼‚å¸¸å›°éš¾ã€‚æ¨¡å‹å‹ç¼©æŠ€æœ¯åº”è¿è€Œç”Ÿï¼Œå®ƒæ—¨åœ¨åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—å‡å°‘æ¨¡å‹çš„å¤§å°ã€å†…å­˜å ç”¨å’Œè®¡ç®—å¤æ‚åº¦ï¼Œä»è€Œä½¿ AI æ›´æ˜“äºå®é™…åº”ç”¨ã€‚

æœ¬æ–‡å°†ä»‹ç»æ¨¡å‹å‹ç¼©çš„åŸºæœ¬æ¦‚å¿µã€å¸¸è§æ–¹æ³•ã€ä¼˜ç¼ºç‚¹ä»¥åŠå®é™…åº”ç”¨åœºæ™¯ã€‚å¦‚æœä½ æ˜¯å¯¹ AI æ„Ÿå…´è¶£çš„å¼€å‘è€…ã€ç ”ç©¶è€…æˆ–ä»ä¸šè€…ï¼Œè¿™ç¯‡æ–‡ç« å°†å¸®åŠ©ä½ å¿«é€Ÿäº†è§£è¿™ä¸€çƒ­é—¨é¢†åŸŸã€‚

## ä»€ä¹ˆæ˜¯æ¨¡å‹å‹ç¼©ï¼Ÿ

æ¨¡å‹å‹ç¼©ï¼ˆModel Compressionï¼‰æ˜¯æŒ‡é€šè¿‡å„ç§ä¼˜åŒ–æ‰‹æ®µï¼Œå°†åŸå§‹æ¨¡å‹çš„ä½“ç§¯å’Œè®¡ç®—é‡å‡å°ï¼ŒåŒæ—¶å°½é‡ä¿ç•™å…¶é¢„æµ‹å‡†ç¡®ç‡çš„è¿‡ç¨‹ã€‚è¿™é¡¹æŠ€æœ¯æºäºç§»åŠ¨è®¡ç®—å’ŒåµŒå…¥å¼ç³»ç»Ÿçš„éœ€æ±‚ï¼Œä½†å¦‚ä»Šå·²å¹¿æ³›åº”ç”¨äºäº‘è®¡ç®—ã€è¾¹ç¼˜è®¡ç®—å’Œå®æ—¶ AI ç³»ç»Ÿã€‚

ä¸ºä»€ä¹ˆéœ€è¦æ¨¡å‹å‹ç¼©ï¼Ÿ
- **èµ„æºé™åˆ¶**ï¼šç§»åŠ¨è®¾å¤‡å†…å­˜å’Œè®¡ç®—èƒ½åŠ›æœ‰é™ï¼Œæ— æ³•è¿è¡Œå¤§å‹æ¨¡å‹ã€‚
- **èƒ½è€—ä¸æˆæœ¬**ï¼šå‡å°‘è®¡ç®—é‡å¯é™ä½èƒ½è€—å’Œéƒ¨ç½²æˆæœ¬ã€‚
- **æ¨ç†é€Ÿåº¦**ï¼šå‹ç¼©åæ¨¡å‹æ¨ç†æ›´å¿«ï¼Œé€‚ç”¨äºå®æ—¶åº”ç”¨å¦‚è‡ªåŠ¨é©¾é©¶æˆ–è¯­éŸ³è¯†åˆ«ã€‚
- **éšç§ä¸å®‰å…¨æ€§**ï¼šå°å‹æ¨¡å‹æ›´å®¹æ˜“åœ¨æœ¬åœ°è¿è¡Œï¼Œé¿å…æ•°æ®ä¼ è¾“é£é™©ã€‚

æ ¹æ®ç»Ÿè®¡ï¼Œå‹ç¼©åçš„æ¨¡å‹ä½“ç§¯å¯ç¼©å° 10-100 å€ï¼Œè€Œæ€§èƒ½æŸå¤±é€šå¸¸æ§åˆ¶åœ¨ 5% ä»¥å†…ã€‚

## å¸¸è§æ¨¡å‹å‹ç¼©æ–¹æ³•

æ¨¡å‹å‹ç¼©æ–¹æ³•å¤šç§å¤šæ ·ï¼Œä¸»è¦åˆ†ä¸ºå››å¤§ç±»ï¼šå‚æ•°å‰ªæã€é‡åŒ–ã€çŸ¥è¯†è’¸é¦å’Œä½ç§©åˆ†è§£ã€‚ä¸‹é¢æˆ‘ä»¬é€ä¸€ä»‹ç»ï¼Œå¹¶é™„ä¸Šç®€å•çš„ä»£ç ç¤ºä¾‹ï¼ˆåŸºäº PyTorch æ¡†æ¶ï¼‰ã€‚

### 1. å‚æ•°å‰ªæï¼ˆPruningï¼‰

å‚æ•°å‰ªææ˜¯é€šè¿‡ç§»é™¤æ¨¡å‹ä¸­ä¸é‡è¦çš„æƒé‡æˆ–ç¥ç»å…ƒæ¥ç®€åŒ–æ¨¡å‹ç»“æ„ã€‚ç±»ä¼¼äºâ€œä¿®å‰ªæ ‘æâ€ï¼Œä¿ç•™æ ¸å¿ƒéƒ¨åˆ†ã€‚

- **å·¥ä½œåŸç†**ï¼šæ ¹æ®æƒé‡çš„å¤§å°ã€æ¢¯åº¦æˆ–é‡è¦æ€§åˆ†æ•°ï¼ˆå¦‚ L1/L2 èŒƒæ•°ï¼‰ï¼Œå°†ä½è´¡çŒ®çš„å‚æ•°è®¾ä¸ºé›¶ã€‚ç„¶åï¼Œé€šè¿‡ç¨€ç–çŸ©é˜µæˆ–ç»“æ„åŒ–å‰ªæè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚
- **ç±»å‹**ï¼š
  - éç»“æ„åŒ–å‰ªæï¼šéšæœºç§»é™¤æƒé‡ï¼Œçµæ´»ä½†ç¡¬ä»¶ä¸å‹å¥½ã€‚
  - ç»“æ„åŒ–å‰ªæï¼šç§»é™¤æ•´ä¸ªé€šé“æˆ–è¿‡æ»¤å™¨ï¼Œæ›´é€‚åˆ GPU/TPU åŠ é€Ÿã€‚
- **ä¼˜ç¼ºç‚¹**ï¼š
  - ä¼˜ç‚¹ï¼šç®€å•æœ‰æ•ˆï¼Œå‹ç¼©ç‡é«˜ï¼ˆå¯è¾¾ 90%ï¼‰ã€‚
  - ç¼ºç‚¹ï¼šå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œéœ€è¦å¤šæ¬¡è¿­ä»£å¾®è°ƒã€‚
- **å·¥å…·ä¸æ¡†æ¶**ï¼šPyTorch çš„ `torch.nn.utils.prune` æ¨¡å—ã€TensorFlow çš„ Model Optimization Toolkitã€‚

ç¤ºä¾‹ï¼šåœ¨ ResNet-50 æ¨¡å‹ä¸Šåº”ç”¨å‰ªæï¼Œå¯å°†å‚æ•°ä» 2500 ä¸‡å‡è‡³ 500 ä¸‡ï¼Œå‡†ç¡®ç‡ä»…ä¸‹é™ 1%ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ PyTorch ä»£ç ç¤ºä¾‹ï¼Œå®ç° L1 éç»“æ„åŒ–å‰ªæï¼š

```python
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune

# å‡è®¾æœ‰ä¸€ä¸ªç®€å•çš„æ¨¡å‹
class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(10, 5)

model = SimpleNet()

# å¯¹å…¨è¿æ¥å±‚åº”ç”¨ L1 å‰ªæï¼Œç§»é™¤ 30% çš„æƒé‡
prune.l1_unstructured(model.fc, name='weight', amount=0.3)

# æŸ¥çœ‹å‰ªæåçš„æ¨¡å‹
print(model.fc.weight)  # ä¼šæ˜¾ç¤ºå¸¦æœ‰æ©ç çš„æƒé‡
```

### 2. é‡åŒ–ï¼ˆQuantizationï¼‰

é‡åŒ–æ˜¯å°†æ¨¡å‹æƒé‡å’Œæ¿€æ´»å€¼ä»é«˜ç²¾åº¦æµ®ç‚¹æ•°ï¼ˆFP32ï¼‰è½¬æ¢ä¸ºä½ç²¾åº¦æ•´æ•°ï¼ˆå¦‚ INT8 æˆ– INT4ï¼‰ï¼Œä»è€Œå‡å°‘å­˜å‚¨å’Œè®¡ç®—éœ€æ±‚ã€‚

- **å·¥ä½œåŸç†**ï¼šä½¿ç”¨é‡åŒ–å‡½æ•°æ˜ å°„æµ®ç‚¹å€¼åˆ°æ•´æ•°èŒƒå›´ï¼Œå¹¶é€šè¿‡æ ¡å‡†æ•°æ®æœ€å°åŒ–é‡åŒ–è¯¯å·®ã€‚å¸¸è§æ–¹æ³•åŒ…æ‹¬åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰ã€‚
- **ç±»å‹**ï¼š
  - å‡åŒ€é‡åŒ–ï¼šå›ºå®šæ­¥é•¿çš„é‡åŒ–ã€‚
  - éå‡åŒ€é‡åŒ–ï¼šæ ¹æ®æ•°æ®åˆ†å¸ƒåŠ¨æ€è°ƒæ•´ã€‚
- **ä¼˜ç¼ºç‚¹**ï¼š
  - ä¼˜ç‚¹ï¼šç¡¬ä»¶å‹å¥½ï¼ˆæ”¯æŒ SIMD æŒ‡ä»¤ï¼‰ï¼Œé€Ÿåº¦æå‡ 2-4 å€ï¼Œä½“ç§¯ç¼©å° 4 å€ã€‚
  - ç¼ºç‚¹ï¼šä½ç²¾åº¦å¯èƒ½å¼•å…¥è¯¯å·®ï¼Œå°¤å…¶åœ¨å¤æ‚æ¨¡å‹ä¸­ã€‚
- **å·¥å…·ä¸æ¡†æ¶**ï¼šTensorFlow Liteã€ONNX Runtime æ”¯æŒé‡åŒ–è½¬æ¢ã€‚

å®é™…æ¡ˆä¾‹ï¼šMobileNet æ¨¡å‹é‡åŒ–åï¼Œåœ¨æ‰‹æœºä¸Šè¿è¡Œé€Ÿåº¦æå‡ 3 å€ï¼Œé€‚ç”¨äºå›¾åƒåˆ†ç±» Appã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ª PyTorch ä»£ç ç¤ºä¾‹ï¼Œå®ç°åè®­ç»ƒé‡åŒ–ï¼š

```python
import torch
import torch.quantization

# å‡è®¾æœ‰ä¸€ä¸ªæµ®ç‚¹æ¨¡å‹
model_fp32 = torch.nn.Sequential(
    torch.nn.Conv2d(1, 20, 5),
    torch.nn.ReLU(),
    torch.nn.Conv2d(20, 64, 5),
    torch.nn.ReLU()
)

# å‡†å¤‡é‡åŒ–
model_fp32.qconfig = torch.quantization.default_qconfig
model_fp32_prepared = torch.quantization.prepare(model_fp32, inplace=False)

# æ ¡å‡†ï¼ˆä½¿ç”¨ä¸€äº›æ•°æ®è¿è¡Œæ¨¡å‹ï¼Œè¿™é‡Œçœç•¥ï¼‰
# model_fp32_prepared(data)

# è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
model_int8 = torch.quantization.convert(model_fp32_prepared)

# æŸ¥çœ‹é‡åŒ–æ¨¡å‹
print(model_int8)
```

### 3. çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰

çŸ¥è¯†è’¸é¦æ˜¯å°†å¤§å‹â€œæ•™å¸ˆâ€æ¨¡å‹çš„çŸ¥è¯†è½¬ç§»åˆ°å°å‹â€œå­¦ç”Ÿâ€æ¨¡å‹ä¸­ã€‚å­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ•™å¸ˆçš„è½¯æ ‡ç­¾ï¼ˆæ¦‚ç‡åˆ†å¸ƒï¼‰è€Œéç¡¬æ ‡ç­¾ã€‚

- **å·¥ä½œåŸç†**ï¼šè®­ç»ƒæ—¶ï¼Œä½¿ç”¨æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºä½œä¸ºå­¦ç”Ÿæ¨¡å‹çš„ç›‘ç£ä¿¡å·ï¼ŒåŠ ä¸Šè’¸é¦æŸå¤±å‡½æ•°ï¼ˆå¦‚ KL æ•£åº¦ï¼‰ã€‚
- **ä¼˜ç¼ºç‚¹**ï¼š
  - ä¼˜ç‚¹ï¼šä¸ä¾èµ–ç‰¹å®šæ¶æ„ï¼Œæ€§èƒ½ä¿ç•™å¥½ï¼Œç”šè‡³æœ‰æ—¶è¶…è¶ŠåŸæ¨¡å‹ã€‚
  - ç¼ºç‚¹ï¼šéœ€è¦è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œè®¡ç®—å¼€é”€å¤§ã€‚
- **å·¥å…·ä¸æ¡†æ¶**ï¼šHugging Face Transformers åº“æ”¯æŒè’¸é¦ã€‚

ç»å…¸æ¡ˆä¾‹ï¼šBERT é€šè¿‡è’¸é¦äº§ç”Ÿ DistilBERTï¼Œå‚æ•°å‡å°‘ 40%ï¼Œé€Ÿåº¦æå‡ 60%ï¼Œæ€§èƒ½ä»…é™ 3%ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ PyTorch ä»£ç ç¤ºä¾‹ï¼Œå®ç°çŸ¥è¯†è’¸é¦æŸå¤±ï¼š

```python
import torch
import torch.nn.functional as F

def distillation_loss(student_logits, teacher_logits, temperature=5.0):
    # è½¯åŒ–æ¦‚ç‡
    soft_teacher = F.softmax(teacher_logits / temperature, dim=1)
    soft_student = F.log_softmax(student_logits / temperature, dim=1)
    # KL æ•£åº¦æŸå¤±
    return F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (temperature ** 2)

# å‡è®¾æ•™å¸ˆå’Œå­¦ç”Ÿè¾“å‡º
teacher_logits = torch.randn(2, 10)
student_logits = torch.randn(2, 10)

loss = distillation_loss(student_logits, teacher_logits)
print(loss)
```

### 4. ä½ç§©åˆ†è§£ï¼ˆLow-Rank Decompositionï¼‰

ä½ç§©åˆ†è§£ä½¿ç”¨çŸ©é˜µåˆ†è§£æŠ€æœ¯ï¼ˆå¦‚ SVD æˆ– Tucker åˆ†è§£ï¼‰å°†é«˜ç»´æƒé‡çŸ©é˜µè¿‘ä¼¼ä¸ºä½ç§©çŸ©é˜µçš„ä¹˜ç§¯ã€‚

- **å·¥ä½œåŸç†**ï¼šå°†æƒé‡çŸ©é˜µ W åˆ†è§£ä¸º U * Vï¼Œå…¶ä¸­ U å’Œ V çš„ç»´åº¦è¿œå°äº Wã€‚
- **ä¼˜ç¼ºç‚¹**ï¼š
  - ä¼˜ç‚¹ï¼šé€‚ç”¨äºå·ç§¯å±‚å’Œå…¨è¿æ¥å±‚ï¼Œå‹ç¼©ç‡é«˜ã€‚
  - ç¼ºç‚¹ï¼šè®¡ç®—åˆ†è§£è¿‡ç¨‹å¤æ‚ï¼Œå¯èƒ½éœ€è¦é‡è®­ç»ƒã€‚
- **å·¥å…·ä¸æ¡†æ¶**ï¼šScikit-learn çš„ SVD æ¨¡å—å¯è¾…åŠ©å®ç°ã€‚

åœ¨ Transformer æ¨¡å‹ä¸­ï¼Œä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰æ˜¯ä¸€ç§æµè¡Œå˜ä½“ï¼Œç”¨äºå¾®è°ƒå¤§æ¨¡å‹è€Œæ— éœ€å…¨å‚æ•°æ›´æ–°ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ª PyTorch ä»£ç ç¤ºä¾‹ï¼Œä½¿ç”¨ SVD è¿›è¡Œä½ç§©åˆ†è§£ï¼š

```python
import torch
from torch.linalg import svd

# å‡è®¾ä¸€ä¸ªæƒé‡çŸ©é˜µ
weight = torch.randn(100, 200)

# SVD åˆ†è§£ï¼Œä¿ç•™å‰ k ä¸ªå¥‡å¼‚å€¼
U, S, Vh = svd(weight)
k = 50  # ä½ç§©è¿‘ä¼¼
low_rank = U[:, :k] @ torch.diag(S[:k]) @ Vh[:k, :]

# æŸ¥çœ‹å½¢çŠ¶
print(low_rank.shape)  # (100, 200)ï¼Œä½†å‚æ•°å‡å°‘
```


## ä¸»æµæ¨¡å‹å‹ç¼©å·¥å…·ä¸æ¡†æ¶

ç°ä»£æ¨¡å‹å‹ç¼©å·²ç»å‘å±•å‡ºå®Œæ•´çš„å·¥å…·ç”Ÿæ€ç³»ç»Ÿï¼Œä»¥ä¸‹æ˜¯æœ€ä¸»æµçš„å·¥å…·ï¼š

### ğŸ”§ ç»¼åˆå‹ç¼©æ¡†æ¶
- **Intel Neural Compressor**: æ”¯æŒPyTorch/TensorFlowçš„ç»Ÿä¸€å‹ç¼©åº“ï¼Œæ”¯æŒé‡åŒ–ã€å‰ªæã€è’¸é¦
- **NVIDIA ModelOpt**: æ”¯æŒTensorRTä¼˜åŒ–çš„ç»Ÿä¸€ä¼˜åŒ–æ¡†æ¶
- **ONNX Runtime**: è·¨å¹³å°æ¨ç†ä¼˜åŒ–å¼•æ“ï¼Œæ”¯æŒINT8/FP16é‡åŒ–
- **Apache TVM**: æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨ï¼Œæ”¯æŒè‡ªåŠ¨ä¼˜åŒ–

### âœ‚ï¸ å‰ªæä¸“ç”¨å·¥å…·
- **PyTorch Pruning**: å†…ç½®çš„`torch.nn.utils.prune`æ¨¡å—
- **NNI (Neural Network Intelligence)**: å¾®è½¯çš„è‡ªåŠ¨åŒ–å‹ç¼©å·¥å…·
- **Torch-Pruning**: é«˜çº§ç»“æ„åŒ–å‰ªæåº“

### âš–ï¸ é‡åŒ–å·¥å…·
- **PyTorch Quantization**: å®˜æ–¹é‡åŒ–æ”¯æŒ
- **TensorRT**: NVIDIAçš„æ¨ç†ä¼˜åŒ–å¼•æ“
- **QNNPACK**: Facebookçš„ç§»åŠ¨ç«¯é‡åŒ–åº“
- **GPTQ**: å¤§æ¨¡å‹é‡åŒ–ä¸“ç”¨å·¥å…·

### ğŸ“ çŸ¥è¯†è’¸é¦æ¡†æ¶
- **Hugging Face Distillation**: Transformersåº“çš„è’¸é¦æ”¯æŒ
- **TinyBERT**: ä¸“ä¸ºBERTè®¾è®¡çš„è’¸é¦æ¡†æ¶
- **MobileBERT**: ç§»åŠ¨ç«¯ä¼˜åŒ–è’¸é¦

## é«˜çº§ä»£ç ç¤ºä¾‹ä¸æœ€ä½³å®è·µ

###  ç»“æ„åŒ–å‰ªæ vs éç»“æ„åŒ–å‰ªæå®Œæ•´å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
from torch.ao.pruning import SaliencyPruner

class ResNetBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.shortcut = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()

    def forward(self, x):
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        return torch.relu(out)

# ç»“æ„åŒ–å‰ªæå®ç°
class StructuredPruner:
    def __init__(self, model, pruning_ratio=0.3):
        self.model = model
        self.pruning_ratio = pruning_ratio
    
    def channel_pruning(self, layer, importance_metric='l1'):
        """é€šé“å‰ªæï¼šç§»é™¤ä¸é‡è¦çš„é€šé“"""
        weight = layer.weight.data
        if importance_metric == 'l1':
            importance = torch.sum(torch.abs(weight), dim=[2, 3])
        elif importance_metric == 'l2':
            importance = torch.sum(weight ** 2, dim=[2, 3]) ** 0.5
        
        # è®¡ç®—è¦ä¿ç•™çš„é€šé“æ•°
        num_channels = weight.shape[0]
        num_keep = int(num_channels * (1 - self.pruning_ratio))
        
        # é€‰æ‹©æœ€é‡è¦çš„é€šé“
        _, top_indices = torch.topk(importance.sum(dim=1), num_keep)
        
        # åˆ›å»ºæ–°çš„æƒé‡
        new_weight = weight[top_indices]
        layer.weight = nn.Parameter(new_weight)
        layer.out_channels = num_keep
        
        return top_indices

# éç»“æ„åŒ–å‰ªæå¯¹æ¯”
class UnstructuredPruner:
    def __init__(self, model, pruning_ratio=0.5):
        self.model = model
        self.pruning_ratio = pruning_ratio
    
    def l1_pruning(self):
        """L1éç»“æ„åŒ–å‰ªæ"""
        for module in self.model.modules():
            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):
                prune.l1_unstructured(module, name='weight', amount=self.pruning_ratio)
        return self.model

# ä½¿ç”¨ç¤ºä¾‹
model = ResNetBlock(64, 128)
pruner = StructuredPruner(model)
pruned_indices = pruner.channel_pruning(model.conv1)
print(f"å‰ªæåé€šé“æ•°: {len(pruned_indices)}")
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
- **ç»“æ„åŒ–å‰ªæ**ï¼šå‚æ•°å‡å°‘40-60%ï¼Œé€Ÿåº¦æå‡2-4xï¼Œç¡¬ä»¶å‹å¥½
- **éç»“æ„åŒ–å‰ªæ**ï¼šå‚æ•°å‡å°‘90%ï¼Œä½†ç¨€ç–çŸ©é˜µè®¡ç®—æ•ˆç‡ä½
- **æ··åˆå‰ªæ**ï¼šå…ˆç»“æ„åŒ–åéç»“æ„åŒ–ï¼Œå¹³è¡¡å‹ç¼©ç‡å’Œæ€§èƒ½

### ä¸‰ç§é‡åŒ–æ–¹å¼å®Œæ•´å®ç°

```python
import torch
from torch.ao.quantization import quantize_dynamic, prepare_qat, convert
from torch.ao.quantization import get_default_qconfig

# åŠ¨æ€é‡åŒ– - é€‚ç”¨äºRNN/LSTM
def apply_dynamic_quantization(model):
    """åŠ¨æ€é‡åŒ–ï¼šè¿è¡Œæ—¶é‡åŒ–æ¿€æ´»"""
    quantized_model = quantize_dynamic(
        model,
        {nn.Linear, nn.LSTM, nn.GRU, nn.Conv2d},
        dtype=torch.qint8
    )
    return quantized_model

# é™æ€é‡åŒ– - é€‚ç”¨äºCNN
def apply_static_quantization(model, calibration_data):
    """é™æ€é‡åŒ–ï¼šé¢„å…ˆé‡åŒ–æƒé‡å’Œæ¿€æ´»"""
    model.eval()
    model.qconfig = get_default_qconfig('fbgemm')
    
    prepared_model = torch.quantization.prepare(model)
    
    # æ ¡å‡† - ä½¿ç”¨ä»£è¡¨æ€§æ•°æ®
    with torch.no_grad():
        for data, _ in calibration_data:
            prepared_model(data)
    
    quantized_model = torch.quantization.convert(prepared_model)
    return quantized_model

# é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ(QAT) - æœ€ä½³æ•ˆæœ
class QATTrainer:
    def __init__(self, model):
        self.model = model
        self.setup_qat()
    
    def setup_qat(self):
        """è®¾ç½®QATé…ç½®"""
        self.model.train()
        self.model.qconfig = get_default_qconfig('fbgemm')
        self.model = torch.ao.quantization.prepare_qat(self.model)
    
    def train_with_qat(self, train_loader, epochs=5, lr=0.01):
        """QATè®­ç»ƒ"""
        optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)
        criterion = nn.CrossEntropyLoss()
        
        for epoch in range(epochs):
            for data, target in train_loader:
                optimizer.zero_grad()
                output = self.model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
        
        return torch.quantization.convert(self.model.eval())

# ä½¿ç”¨ç¤ºä¾‹
class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, 3)
        self.conv2 = nn.Conv2d(64, 128, 3)
        self.fc = nn.Linear(128 * 6 * 6, 10)
    
    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.conv2(x))
        x = torch.max_pool2d(x, 2)
        x = x.view(-1, 128 * 6 * 6)
        x = self.fc(x)
        return x

model = SimpleCNN()
```

**é‡åŒ–æ€§èƒ½å¯¹æ¯”è¡¨**ï¼š

| é‡åŒ–ç±»å‹ | å‚æ•°å‡å°‘ | æ¨ç†é€Ÿåº¦æå‡ | å‡†ç¡®ç‡æŸå¤± | é€‚ç”¨åœºæ™¯ |
|---------|----------|-------------|-----------|----------|
| åŠ¨æ€é‡åŒ– | 75% | 3-4x | <1% | RNN/LSTM |
| é™æ€é‡åŒ– | 75% | 4x | <2% | CNNæ¨¡å‹ |
| QATé‡åŒ– | 75% | 3x | <0.5% | é«˜ç²¾åº¦è¦æ±‚ |
| INT4é‡åŒ– | 87.5% | 8x | 2-3% | æè‡´å‹ç¼© |

### é«˜çº§çŸ¥è¯†è’¸é¦æŠ€æœ¯

```python
import torch.nn.functional as F
from transformers import AutoModel, AutoTokenizer

class AdvancedDistiller:
    def __init__(self, teacher_model, student_model, temperature=4.0):
        self.teacher = teacher_model
        self.student = student_model
        self.temperature = temperature
        
    def attention_distillation(self, teacher_attn, student_attn):
        """æ³¨æ„åŠ›è’¸é¦ï¼šåŒ¹é…æ³¨æ„åŠ›æƒé‡"""
        loss = 0
        for t_attn, s_attn in zip(teacher_attn, student_attn):
            t_probs = F.softmax(t_attn / self.temperature, dim=-1)
            s_probs = F.log_softmax(s_attn / self.temperature, dim=-1)
            loss += F.kl_div(s_probs, t_probs, reduction='batchmean')
        return loss
    
    def hidden_state_distillation(self, teacher_hidden, student_hidden):
        """éšè—çŠ¶æ€è’¸é¦ï¼šåŒ¹é…ä¸­é—´å±‚è¡¨ç¤º"""
        loss = 0
        for t_hidden, s_hidden in zip(teacher_hidden, student_hidden):
            loss += F.mse_loss(s_hidden, t_hidden)
        return loss
    
    def multi_teacher_distillation(self, outputs, teacher_outputs):
        """å¤šæ•™å¸ˆè’¸é¦ï¼šé›†æˆå¤šä¸ªæ•™å¸ˆ"""
        teacher_logits = torch.stack([t['logits'] for t in teacher_outputs])
        avg_teacher = torch.mean(teacher_logits, dim=0)
        
        soft_targets = F.softmax(avg_teacher / self.temperature, dim=-1)
        soft_outputs = F.log_softmax(outputs / self.temperature, dim=-1)
        distill_loss = F.kl_div(soft_outputs, soft_targets, reduction='batchmean')
        
        return distill_loss

# TinyBERTè’¸é¦å®Œæ•´å®ç°
class TinyBERTDistillation:
    def __init__(self, teacher_name="bert-base-uncased"):
        self.teacher = AutoModel.from_pretrained(teacher_name)
        self.student = AutoModel.from_pretrained("huawei-noah/TinyBERT_6L_768D")
        self.tokenizer = AutoTokenizer.from_pretrained(teacher_name)
    
    def distill_step(self, texts, labels):
        """å•æ­¥è’¸é¦è®­ç»ƒ"""
        inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
        
        # æ•™å¸ˆæ¨¡å‹è¾“å‡ºï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
        with torch.no_grad():
            teacher_outputs = self.teacher(**inputs)
        
        # å­¦ç”Ÿæ¨¡å‹è¾“å‡º
        student_outputs = self.student(**inputs)
        
        # è®¡ç®—è’¸é¦æŸå¤±
        distill_loss = self.attention_distillation(
            teacher_outputs.attentions, 
            student_outputs.attentions
        )
        
        return distill_loss

# ä½¿ç”¨ç¤ºä¾‹
distiller = TinyBERTDistillation()
```

### LoRA/AdaLoRA/QLoRA å®Œæ•´å®ç°

```python
from peft import LoraConfig, get_peft_model, AdaLoraConfig
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

class LoRATrainer:
    def __init__(self, model_name="microsoft/DialoGPT-medium"):
        self.model_name = model_name
    
    def setup_lora(self, rank=8, alpha=32):
        """æ ‡å‡†LoRAé…ç½®"""
        lora_config = LoraConfig(
            r=rank,
            lora_alpha=alpha,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            lora_dropout=0.1,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        model = AutoModelForCausalLM.from_pretrained(self.model_name)
        lora_model = get_peft_model(model, lora_config)
        return lora_model
    
    def setup_qlora(self, load_in_4bit=True, r=64):
        """QLoRAé‡åŒ–é…ç½®"""
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=load_in_4bit,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=quantization_config
        )
        
        qlora_config = LoraConfig(
            r=r,
            lora_alpha=16,
            target_modules=["q_proj", "k_proj", "v_proj"],
            lora_dropout=0.05
        )
        
        return get_peft_model(model, qlora_config)

# AdaLoRAé…ç½®
class AdaLoRATrainer:
    def __init__(self, model_name):
        self.model_name = model_name
    
    def setup_adalora(self, initial_rank=16, target_rank=8):
        """AdaLoRAè‡ªé€‚åº”ç§©é…ç½®"""
        adalora_config = AdaLoraConfig(
            r=initial_rank,
            lora_alpha=32,
            target_r=target_rank,
            init_r=initial_rank,
            tinit=200,
            tfinal=1000,
            deltaT=10,
            orth_reg_weight=0.5,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
        )
        
        model = AutoModelForCausalLM.from_pretrained(self.model_name)
        return get_peft_model(model, adalora_config)

# æ€§èƒ½å¯¹æ¯”
lora_performance = {
    "LoRA": {"å‚æ•°å‡å°‘": "99%", "å†…å­˜å‡å°‘": "95%", "å‡†ç¡®ç‡": "ä¿æŒ99%+"},
    "QLoRA": {"å‚æ•°å‡å°‘": "99.5%", "å†…å­˜å‡å°‘": "97%", "æ¨ç†é€Ÿåº¦": "æå‡3x"},
    "AdaLoRA": {"å‚æ•°å‡å°‘": "99%", "å†…å­˜å‡å°‘": "96%", "è‡ªé€‚åº”": "åŠ¨æ€è°ƒæ•´ç§©"}
}
```

## æ··åˆå‹ç¼©ç­–ç•¥ä¸å®æˆ˜éƒ¨ç½²

### ç»„åˆå‹ç¼©ç­–ç•¥

```python
class HybridCompressor:
    def __init__(self, model):
        self.model = model
    
    def pruning_then_quantization(self, pruning_ratio=0.5):
        """å…ˆå‰ªæåé‡åŒ–"""
        # æ­¥éª¤1: ç»“æ„åŒ–å‰ªæ
        pruned_model = self.apply_structured_pruning(pruning_ratio)
        
        # æ­¥éª¤2: é‡åŒ–
        quantized_model = self.apply_static_quantization(pruned_model)
        
        return quantized_model
    
    def quantization_then_distillation(self, teacher_model):
        """é‡åŒ–+è’¸é¦ç»„åˆ"""
        quantized_teacher = self.quantize_model(teacher_model)
        distilled_model = self.distill_to_student(quantized_teacher)
        return distilled_model

# å®æˆ˜æ€§èƒ½æ•°æ®
hybrid_performance = {
    "ResNet50 + å‰ªæ50% + é‡åŒ–": {
        "å‚æ•°å‡å°‘": "90%",
        "æ¨ç†é€Ÿåº¦æå‡": "10x",
        "å‡†ç¡®ç‡æŸå¤±": "2%",
        "å†…å­˜å ç”¨": "å‡å°‘85%"
    },
    "BERT + LoRA + é‡åŒ–": {
        "å‚æ•°å‡å°‘": "99.5%",
        "æ¨ç†é€Ÿåº¦æå‡": "3x",
        "å‡†ç¡®ç‡æŸå¤±": "1%"
    }
}
```

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ä¼˜åŒ–

```python
# TensorRTä¼˜åŒ–
import tensorrt as trt

def optimize_for_tensorrt(model, input_shape):
    """TensorRTä¼˜åŒ–æ¨¡å‹éƒ¨ç½²"""
    model.eval()
    dummy_input = torch.randn(input_shape)
    
    # å¯¼å‡ºONNX
    torch.onnx.export(model, dummy_input, "model.onnx", 
                     export_params=True, opset_version=11)
    
    # TensorRTä¼˜åŒ–æµç¨‹
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)
    network = builder.create_network()
    parser = trt.OnnxParser(network, logger)
    
    with open("model.onnx", "rb") as model_file:
        parser.parse(model_file.read())
    
    # é…ç½®ä¼˜åŒ–å‚æ•°
    config = builder.create_builder_config()
    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # 1GB
    
    # ç”Ÿæˆä¼˜åŒ–å¼•æ“
    engine = builder.build_engine(network, config)
    return engine

# ONNX Runtimeä¼˜åŒ–
from onnxruntime.quantization import quantize_dynamic, QuantType

def optimize_for_onnx(model_path):
    """ONNX Runtimeä¼˜åŒ–"""
    quantized_model = quantize_dynamic(
        model_path,
        model_path.replace('.onnx', '_quantized.onnx'),
        weight_type=QuantType.QInt8
    )
    return quantized_model

# Dockeréƒ¨ç½²ç¤ºä¾‹
'''
FROM pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY model_compressed.pt .
COPY inference.py .
CMD ["python", "inference.py"]
'''
```

## æœ€ç»ˆæ€§èƒ½å¯¹æ¯”è¡¨

| å‹ç¼©æ–¹æ³• | å‚æ•°å‡å°‘ | æ¨ç†é€Ÿåº¦æå‡ | å‡†ç¡®ç‡æŸå¤± | é€‚ç”¨åœºæ™¯ | å·¥å…·æ¨è |
|---------|----------|-------------|-----------|----------|----------|
| ç»“æ„åŒ–å‰ªæ | 40-90% | 2-10x | <2% | CNNæ¨¡å‹ | PyTorch Pruning |
| åŠ¨æ€é‡åŒ– | 75% | 3-4x | <1% | RNN/LSTM | torch.quantization |
| é™æ€é‡åŒ– | 75% | 4x | <2% | CNNæ¨¡å‹ | Intel Neural Compressor |
| QATé‡åŒ– | 75% | 3x | <0.5% | é«˜ç²¾åº¦è¦æ±‚ | PyTorch QAT |
| LoRAå¾®è°ƒ | 99% | - | <1% | å¤§æ¨¡å‹å¾®è°ƒ | PEFTåº“ |
| çŸ¥è¯†è’¸é¦ | 40-60% | 2-3x | <3% | æ¨¡å‹å‹ç¼© | Hugging Face |
| INT4é‡åŒ– | 87.5% | 8x | 2-3% | æè‡´å‹ç¼© | GPTQ |
| æ··åˆç­–ç•¥ | 90%+ | 10x+ | <3% | æè‡´ä¼˜åŒ– | ç»„åˆæ–¹æ³• |

## æœ€ä½³å®è·µ

### ğŸ“Š é€‰æ‹©å†³ç­–æ ‘

```
æ¨¡å‹ç±»å‹?
â”œâ”€â”€ LLM (GPT/BERT)
â”‚   â”œâ”€â”€ å¾®è°ƒéœ€æ±‚ â†’ LoRA/QLoRA
â”‚   â””â”€â”€ æ¨ç†ä¼˜åŒ– â†’ INT8é‡åŒ– + çŸ¥è¯†è’¸é¦
â”œâ”€â”€ CNN (ResNet/EfficientNet)
â”‚   â”œâ”€â”€ è¾¹ç¼˜éƒ¨ç½² â†’ ç»“æ„åŒ–å‰ªæ + é™æ€é‡åŒ–
â”‚   â””â”€â”€ äº‘ç«¯éƒ¨ç½² â†’ åŠ¨æ€é‡åŒ– + TensorRT
â””â”€â”€ RNN/LSTM
    â”œâ”€â”€ ç§»åŠ¨ç«¯ â†’ åŠ¨æ€é‡åŒ–
    â””â”€â”€ æœåŠ¡å™¨ â†’ é™æ€é‡åŒ– + å‰ªæ
```

### âš ï¸ å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ

1. **è¿‡åº¦å‹ç¼©**: å‡†ç¡®ç‡ä¸‹é™>5%
   - è§£å†³æ–¹æ¡ˆï¼šé€æ­¥å‹ç¼©ï¼Œæ¯æ­¥éªŒè¯å‡†ç¡®ç‡
   ```python
   def gradual_compression(model, target_accuracy):
       current_accuracy = evaluate_model(model)
       while current_accuracy > target_accuracy:
           model = compress_model(model)
           current_accuracy = evaluate_model(model)
       return model

    # ä¼ªä»£ç ç¤ºä¾‹
    for layer in model.layers:
        for prune_ratio in [0.1, 0.2, 0.3, 0.4, 0.5]:
            # å¯¹è¯¥å±‚åº”ç”¨å‰ªæ
            prune_layer(layer, prune_ratio)
            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ç²¾åº¦
            accuracy = evaluate(model, val_data)
            # è®°å½•ç²¾åº¦æŸå¤±
            accuracy_drop = original_accuracy - accuracy
            # æ‰¾åˆ°ç²¾åº¦æŸå¤±å¯æ¥å—çš„æœ€å¤§å‰ªææ¯”ä¾‹   
            if accuracy_drop <= max_drop:
                max_drop = accuracy_drop
                best_prune_ratio = prune_ratio
            # åº”ç”¨æœ€ä½³å‰ªææ¯”ä¾‹
            prune_layer(layer, best_prune_ratio)
    
    # é€æ­¥å‰ªæç¤ºä¾‹
    for step in range(3):
        # æ¯æ¬¡å‰ªæ10%
        prune_model(model, amount=0.1)
        # å¾®è°ƒæ¢å¤ç²¾åº¦
        fine_tune(model, lr=1e-4, epochs=5)
  
  ```

2. **ç¡¬ä»¶ä¸å…¼å®¹**: æŸäº›è®¾å¤‡ä¸æ”¯æŒé‡åŒ–
   - è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ONNX Runtimeç»Ÿä¸€æ ¼å¼

3. **å†…å­˜å³°å€¼**: è®­ç»ƒæ—¶å†…å­˜ä¸è¶³
   - è§£å†³æ–¹æ¡ˆï¼šæ¢¯åº¦ç´¯ç§¯ + æ··åˆç²¾åº¦è®­ç»ƒ

4. **æ ¡å‡†æ•°æ®ä¸è¶³**: é‡åŒ–åæ•ˆæœå·®
   - è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨1000+ä»£è¡¨æ€§æ ·æœ¬æ ¡å‡†

5. **éƒ¨ç½²å¤æ‚åº¦**: éƒ¨ç½²è¿‡ç¨‹ç¹ç
   - è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ç»Ÿä¸€éƒ¨ç½²æ¡†æ¶ï¼Œå¦‚ONNX Runtime

## æ¨¡å‹å‹ç¼©çš„å®é™…åº”ç”¨ä¸æŒ‘æˆ˜

æ¨¡å‹å‹ç¼©å·²åœ¨å¤šä¸ªé¢†åŸŸè½åœ°ï¼š
- **ç§»åŠ¨ AI**ï¼šå¦‚ Siri æˆ– Google Assistantï¼Œä½¿ç”¨é‡åŒ–æ¨¡å‹å®ç°æœ¬åœ°è¯­éŸ³è¯†åˆ«ã€‚
- **è¾¹ç¼˜è®¡ç®—**ï¼šè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­ï¼Œå‹ç¼© YOLO æ¨¡å‹ç”¨äºå®æ—¶ç‰©ä½“æ£€æµ‹ã€‚
- **äº‘æœåŠ¡**ï¼šAWS æˆ– Azure é€šè¿‡å‹ç¼©é™ä½æ¨ç†æˆæœ¬ã€‚
- **å¼€æºç¤¾åŒº**ï¼šHugging Face çš„ Model Hub æä¾›å¤§é‡å‹ç¼©æ¨¡å‹ï¼Œå¦‚ TinyBERTã€‚

ç„¶è€Œï¼ŒæŒ‘æˆ˜çŠ¹å­˜ï¼š
- **æ€§èƒ½æƒè¡¡**ï¼šè¿‡åº¦å‹ç¼©å¯èƒ½å¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚
- **ç¡¬ä»¶å…¼å®¹**ï¼šä¸åŒè®¾å¤‡å¯¹å‹ç¼©æ–¹æ³•çš„æ”¯æŒåº¦ä¸åŒã€‚
- **å®‰å…¨æ€§**ï¼šå‹ç¼©æ¨¡å‹å¯èƒ½æ›´å®¹æ˜“è¢«æ”»å‡»ï¼Œå¦‚æ¨¡å‹çªƒå–ã€‚

ç ”ç©¶è€…æ­£æ¢ç´¢æ··åˆæ–¹æ³•ï¼ˆå¦‚å‰ªæ + é‡åŒ–ï¼‰ï¼Œæˆ–è‡ªåŠ¨åŒ–å‹ç¼©æ¡†æ¶ï¼ˆå¦‚ AutoMLï¼‰æ¥åº”å¯¹è¿™äº›é—®é¢˜ã€‚

## ç»“è®º

æ¨¡å‹å‹ç¼©æ˜¯ AI èµ°å‘æ™®æƒ çš„å…³é”®æŠ€æœ¯ï¼Œå®ƒä¸ä»…è§£å†³äº†èµ„æºç“¶é¢ˆï¼Œè¿˜æ¨åŠ¨äº†ç»¿è‰²è®¡ç®—çš„å‘å±•ã€‚æœªæ¥ï¼Œéšç€ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰å’Œç¡¬ä»¶ååŒè®¾è®¡çš„è¿›æ­¥ï¼Œå‹ç¼©æŠ€æœ¯å°†æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆã€‚å¦‚æœä½ æ­£ä»äº‹ AI é¡¹ç›®ï¼Œä¸å¦¨ä»ç®€å•çš„æ–¹æ³•å…¥æ‰‹ï¼Œå°è¯•å‹ç¼©ä½ çš„æ¨¡å‹â€”â€”æˆ–è®¸ä¼šå¸¦æ¥æƒŠå–œï¼

å¦‚æœæœ¬æ–‡å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ç‚¹èµã€æ”¶è—æˆ–è¯„è®ºåˆ†äº«ä½ çš„ç»éªŒã€‚å‚è€ƒèµ„æ–™ä¸»è¦æ¥è‡ª arXiv è®ºæ–‡å’Œå®˜æ–¹æ–‡æ¡£ï¼Œå¦‚æœ‰å…´è¶£å¯è¿›ä¸€æ­¥é˜…è¯»ã€ŠNeural Network Compression Frameworkã€‹ç­‰ã€‚

ï¼ˆæ³¨ï¼šæœ¬æ–‡åŸºäºå…¬å¼€çŸ¥è¯†æ’°å†™ï¼Œä»£ç ç¤ºä¾‹ä»…ä¾›å‚è€ƒï¼Œå¦‚éœ€å®é™…åº”ç”¨ï¼Œè¯·ç¡®ä¿ç¯å¢ƒé…ç½®æ­£ç¡®ï¼Œå¹¶è¿›è¡Œå®Œæ•´æµ‹è¯•ã€‚ï¼‰
