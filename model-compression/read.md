# æ¨¡å‹å‹ç¼©æŠ€æœ¯

## å¼•è¨€

åœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰è¿…çŒ›å‘å±•çš„ä»Šå¤©ï¼Œå¤§å‹æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚ GPT ç³»åˆ—ã€BERT ç­‰å·²æˆä¸ºä¸»æµã€‚è¿™äº›æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸå±•ç°å‡ºæƒŠäººçš„æ€§èƒ½ï¼Œä½†éšä¹‹è€Œæ¥çš„æ˜¯åºå¤§çš„å‚æ•°é‡å’Œè®¡ç®—èµ„æºéœ€æ±‚ã€‚ä¾‹å¦‚ï¼ŒGPT-3 æ¨¡å‹çš„å‚æ•°è§„æ¨¡é«˜è¾¾ 1750 äº¿ï¼Œè¿™ä½¿å¾—å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚æ‰‹æœºã€æ™ºèƒ½å®¶å±…ï¼‰ä¸Šçš„éƒ¨ç½²å˜å¾—å¼‚å¸¸å›°éš¾ã€‚æ¨¡å‹å‹ç¼©æŠ€æœ¯åº”è¿è€Œç”Ÿï¼Œå®ƒæ—¨åœ¨åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—å‡å°‘æ¨¡å‹çš„å¤§å°ã€å†…å­˜å ç”¨å’Œè®¡ç®—å¤æ‚åº¦ï¼Œä»è€Œä½¿ AI æ›´æ˜“äºå®é™…åº”ç”¨ã€‚

æœ¬æ–‡å°†ä»‹ç»æ¨¡å‹å‹ç¼©çš„åŸºæœ¬æ¦‚å¿µã€å¸¸è§æ–¹æ³•ã€ä¼˜ç¼ºç‚¹ä»¥åŠå®é™…åº”ç”¨åœºæ™¯ã€‚å¦‚æœä½ æ˜¯å¯¹ AI æ„Ÿå…´è¶£çš„å¼€å‘è€…ã€ç ”ç©¶è€…æˆ–ä»ä¸šè€…ï¼Œè¿™ç¯‡æ–‡ç« å°†å¸®åŠ©ä½ å¿«é€Ÿäº†è§£è¿™ä¸€çƒ­é—¨é¢†åŸŸã€‚

## ä»€ä¹ˆæ˜¯æ¨¡å‹å‹ç¼©

æ¨¡å‹å‹ç¼©ï¼ˆModel Compressionï¼‰æ˜¯æŒ‡é€šè¿‡å„ç§ä¼˜åŒ–æ‰‹æ®µï¼Œå°†åŸå§‹æ¨¡å‹çš„ä½“ç§¯å’Œè®¡ç®—é‡å‡å°ï¼ŒåŒæ—¶å°½é‡ä¿ç•™å…¶é¢„æµ‹å‡†ç¡®ç‡çš„è¿‡ç¨‹ã€‚è¿™é¡¹æŠ€æœ¯æºäºç§»åŠ¨è®¡ç®—å’ŒåµŒå…¥å¼ç³»ç»Ÿçš„éœ€æ±‚ï¼Œä½†å¦‚ä»Šå·²å¹¿æ³›åº”ç”¨äºäº‘è®¡ç®—ã€è¾¹ç¼˜è®¡ç®—å’Œå®æ—¶ AI ç³»ç»Ÿã€‚

ä¸ºä»€ä¹ˆéœ€è¦æ¨¡å‹å‹ç¼©ï¼Ÿ
- **èµ„æºé™åˆ¶**ï¼šç§»åŠ¨è®¾å¤‡å†…å­˜å’Œè®¡ç®—èƒ½åŠ›æœ‰é™ï¼Œæ— æ³•è¿è¡Œå¤§å‹æ¨¡å‹ã€‚
- **èƒ½è€—ä¸æˆæœ¬**ï¼šå‡å°‘è®¡ç®—é‡å¯é™ä½èƒ½è€—å’Œéƒ¨ç½²æˆæœ¬ã€‚
- **æ¨ç†é€Ÿåº¦**ï¼šå‹ç¼©åæ¨¡å‹æ¨ç†æ›´å¿«ï¼Œé€‚ç”¨äºå®æ—¶åº”ç”¨å¦‚è‡ªåŠ¨é©¾é©¶æˆ–è¯­éŸ³è¯†åˆ«ã€‚
- **éšç§ä¸å®‰å…¨æ€§**ï¼šå°å‹æ¨¡å‹æ›´å®¹æ˜“åœ¨æœ¬åœ°è¿è¡Œï¼Œé¿å…æ•°æ®ä¼ è¾“é£é™©ã€‚

æ ¹æ®ç»Ÿè®¡ï¼Œå‹ç¼©åçš„æ¨¡å‹ä½“ç§¯å¯ç¼©å° 10-100 å€ï¼Œè€Œæ€§èƒ½æŸå¤±é€šå¸¸æ§åˆ¶åœ¨ 5% ä»¥å†…ã€‚

## å¸¸è§æ¨¡å‹å‹ç¼©æ–¹æ³•

æ¨¡å‹å‹ç¼©æ–¹æ³•å¤šç§å¤šæ ·ï¼Œä¸»è¦åˆ†ä¸ºå››å¤§ç±»ï¼šå‚æ•°å‰ªæã€é‡åŒ–ã€çŸ¥è¯†è’¸é¦å’Œä½ç§©åˆ†è§£ã€‚ä¸‹é¢æˆ‘ä»¬é€ä¸€ä»‹ç»ï¼Œå¹¶é™„ä¸Šç®€å•çš„ä»£ç ç¤ºä¾‹ï¼ˆåŸºäº PyTorch æ¡†æ¶ï¼‰ã€‚

### 1. å‚æ•°å‰ªæï¼ˆPruningï¼‰

å‚æ•°å‰ªææ˜¯é€šè¿‡ç§»é™¤æ¨¡å‹ä¸­ä¸é‡è¦çš„æƒé‡æˆ–ç¥ç»å…ƒæ¥ç®€åŒ–æ¨¡å‹ç»“æ„ã€‚ç±»ä¼¼äºâ€œä¿®å‰ªæ ‘æâ€ï¼Œä¿ç•™æ ¸å¿ƒéƒ¨åˆ†ã€‚

- **å·¥ä½œåŸç†**ï¼šæ ¹æ®æƒé‡çš„å¤§å°ã€æ¢¯åº¦æˆ–é‡è¦æ€§åˆ†æ•°ï¼ˆå¦‚ L1/L2 èŒƒæ•°ï¼‰ï¼Œå°†ä½è´¡çŒ®çš„å‚æ•°è®¾ä¸ºé›¶ã€‚ç„¶åï¼Œé€šè¿‡ç¨€ç–çŸ©é˜µæˆ–ç»“æ„åŒ–å‰ªæè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚
- **ç±»å‹**ï¼š
  - éç»“æ„åŒ–å‰ªæï¼šéšæœºç§»é™¤æƒé‡ï¼Œçµæ´»ä½†ç¡¬ä»¶ä¸å‹å¥½ã€‚
  - ç»“æ„åŒ–å‰ªæï¼šç§»é™¤æ•´ä¸ªé€šé“æˆ–è¿‡æ»¤å™¨ï¼Œæ›´é€‚åˆ GPU/TPU åŠ é€Ÿã€‚
- **ä¼˜ç¼ºç‚¹**ï¼š
  - ä¼˜ç‚¹ï¼šç®€å•æœ‰æ•ˆï¼Œå‹ç¼©ç‡é«˜ï¼ˆå¯è¾¾ 90%ï¼‰ã€‚
  - ç¼ºç‚¹ï¼šå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œéœ€è¦å¤šæ¬¡è¿­ä»£å¾®è°ƒã€‚
- **å·¥å…·ä¸æ¡†æ¶**ï¼šPyTorch çš„ `torch.nn.utils.prune` æ¨¡å—ã€TensorFlow çš„ Model Optimization Toolkitã€‚

ç¤ºä¾‹ï¼šåœ¨ ResNet-50 æ¨¡å‹ä¸Šåº”ç”¨å‰ªæï¼Œå¯å°†å‚æ•°ä» 2500 ä¸‡å‡è‡³ 500 ä¸‡ï¼Œå‡†ç¡®ç‡ä»…ä¸‹é™ 1%ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ PyTorch ä»£ç ç¤ºä¾‹ï¼Œå®ç° L1 éç»“æ„åŒ–å‰ªæï¼š

```python
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune

# å‡è®¾æœ‰ä¸€ä¸ªç®€å•çš„æ¨¡å‹
class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(10, 5)

model = SimpleNet()

# å¯¹å…¨è¿æ¥å±‚åº”ç”¨ L1 å‰ªæï¼Œç§»é™¤ 30% çš„æƒé‡
prune.l1_unstructured(model.fc, name='weight', amount=0.3)

# æŸ¥çœ‹å‰ªæåçš„æ¨¡å‹
print(model.fc.weight)  # ä¼šæ˜¾ç¤ºå¸¦æœ‰æ©ç çš„æƒé‡
```

### 2. é‡åŒ–ï¼ˆQuantizationï¼‰

é‡åŒ–æ˜¯å°†æ¨¡å‹æƒé‡å’Œæ¿€æ´»å€¼ä»é«˜ç²¾åº¦æµ®ç‚¹æ•°ï¼ˆFP32ï¼‰è½¬æ¢ä¸ºä½ç²¾åº¦æ•´æ•°ï¼ˆå¦‚ INT8 æˆ– INT4ï¼‰ï¼Œä»è€Œå‡å°‘å­˜å‚¨å’Œè®¡ç®—éœ€æ±‚ã€‚

- **å·¥ä½œåŸç†**ï¼šä½¿ç”¨é‡åŒ–å‡½æ•°æ˜ å°„æµ®ç‚¹å€¼åˆ°æ•´æ•°èŒƒå›´ï¼Œå¹¶é€šè¿‡æ ¡å‡†æ•°æ®æœ€å°åŒ–é‡åŒ–è¯¯å·®ã€‚å¸¸è§æ–¹æ³•åŒ…æ‹¬åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰ã€‚
- **ç±»å‹**ï¼š
  - å‡åŒ€é‡åŒ–ï¼šå›ºå®šæ­¥é•¿çš„é‡åŒ–ã€‚
  - éå‡åŒ€é‡åŒ–ï¼šæ ¹æ®æ•°æ®åˆ†å¸ƒåŠ¨æ€è°ƒæ•´ã€‚
- **ä¼˜ç¼ºç‚¹**ï¼š
  - ä¼˜ç‚¹ï¼šç¡¬ä»¶å‹å¥½ï¼ˆæ”¯æŒ SIMD æŒ‡ä»¤ï¼‰ï¼Œé€Ÿåº¦æå‡ 2-4 å€ï¼Œä½“ç§¯ç¼©å° 4 å€ã€‚
  - ç¼ºç‚¹ï¼šä½ç²¾åº¦å¯èƒ½å¼•å…¥è¯¯å·®ï¼Œå°¤å…¶åœ¨å¤æ‚æ¨¡å‹ä¸­ã€‚
- **å·¥å…·ä¸æ¡†æ¶**ï¼šTensorFlow Liteã€ONNX Runtime æ”¯æŒé‡åŒ–è½¬æ¢ã€‚

å®é™…æ¡ˆä¾‹ï¼šMobileNet æ¨¡å‹é‡åŒ–åï¼Œåœ¨æ‰‹æœºä¸Šè¿è¡Œé€Ÿåº¦æå‡ 3 å€ï¼Œé€‚ç”¨äºå›¾åƒåˆ†ç±» Appã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ª PyTorch ä»£ç ç¤ºä¾‹ï¼Œå®ç°åè®­ç»ƒé‡åŒ–ï¼š

```python
import torch
import torch.quantization

# å‡è®¾æœ‰ä¸€ä¸ªæµ®ç‚¹æ¨¡å‹
model_fp32 = torch.nn.Sequential(
    torch.nn.Conv2d(1, 20, 5),
    torch.nn.ReLU(),
    torch.nn.Conv2d(20, 64, 5),
    torch.nn.ReLU()
)

# å‡†å¤‡é‡åŒ–
model_fp32.qconfig = torch.quantization.default_qconfig
model_fp32_prepared = torch.quantization.prepare(model_fp32, inplace=False)

# æ ¡å‡†ï¼ˆä½¿ç”¨ä¸€äº›æ•°æ®è¿è¡Œæ¨¡å‹ï¼Œè¿™é‡Œçœç•¥ï¼‰
# model_fp32_prepared(data)

# è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
model_int8 = torch.quantization.convert(model_fp32_prepared)

# æŸ¥çœ‹é‡åŒ–æ¨¡å‹
print(model_int8)
```

### 3. çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰

çŸ¥è¯†è’¸é¦æ˜¯å°†å¤§å‹â€œæ•™å¸ˆâ€æ¨¡å‹çš„çŸ¥è¯†è½¬ç§»åˆ°å°å‹â€œå­¦ç”Ÿâ€æ¨¡å‹ä¸­ã€‚å­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ•™å¸ˆçš„è½¯æ ‡ç­¾ï¼ˆæ¦‚ç‡åˆ†å¸ƒï¼‰è€Œéç¡¬æ ‡ç­¾ã€‚

- **å·¥ä½œåŸç†**ï¼šè®­ç»ƒæ—¶ï¼Œä½¿ç”¨æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºä½œä¸ºå­¦ç”Ÿæ¨¡å‹çš„ç›‘ç£ä¿¡å·ï¼ŒåŠ ä¸Šè’¸é¦æŸå¤±å‡½æ•°ï¼ˆå¦‚ KL æ•£åº¦ï¼‰ã€‚
- **ä¼˜ç¼ºç‚¹**ï¼š
  - ä¼˜ç‚¹ï¼šä¸ä¾èµ–ç‰¹å®šæ¶æ„ï¼Œæ€§èƒ½ä¿ç•™å¥½ï¼Œç”šè‡³æœ‰æ—¶è¶…è¶ŠåŸæ¨¡å‹ã€‚
  - ç¼ºç‚¹ï¼šéœ€è¦è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œè®¡ç®—å¼€é”€å¤§ã€‚
- **å·¥å…·ä¸æ¡†æ¶**ï¼šHugging Face Transformers åº“æ”¯æŒè’¸é¦ã€‚

ç»å…¸æ¡ˆä¾‹ï¼šBERT é€šè¿‡è’¸é¦äº§ç”Ÿ DistilBERTï¼Œå‚æ•°å‡å°‘ 40%ï¼Œé€Ÿåº¦æå‡ 60%ï¼Œæ€§èƒ½ä»…é™ 3%ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ PyTorch ä»£ç ç¤ºä¾‹ï¼Œå®ç°çŸ¥è¯†è’¸é¦æŸå¤±ï¼š

```python
import torch
import torch.nn.functional as F

def distillation_loss(student_logits, teacher_logits, temperature=5.0):
    # è½¯åŒ–æ¦‚ç‡
    soft_teacher = F.softmax(teacher_logits / temperature, dim=1)
    soft_student = F.log_softmax(student_logits / temperature, dim=1)
    # KL æ•£åº¦æŸå¤±
    return F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (temperature ** 2)

# å‡è®¾æ•™å¸ˆå’Œå­¦ç”Ÿè¾“å‡º
teacher_logits = torch.randn(2, 10)
student_logits = torch.randn(2, 10)

loss = distillation_loss(student_logits, teacher_logits)
print(loss)
```

### 4. ä½ç§©åˆ†è§£ï¼ˆLow-Rank Decompositionï¼‰

ä½ç§©åˆ†è§£ä½¿ç”¨çŸ©é˜µåˆ†è§£æŠ€æœ¯ï¼ˆå¦‚ SVD æˆ– Tucker åˆ†è§£ï¼‰å°†é«˜ç»´æƒé‡çŸ©é˜µè¿‘ä¼¼ä¸ºä½ç§©çŸ©é˜µçš„ä¹˜ç§¯ã€‚

- **å·¥ä½œåŸç†**ï¼šå°†æƒé‡çŸ©é˜µ W åˆ†è§£ä¸º U * Vï¼Œå…¶ä¸­ U å’Œ V çš„ç»´åº¦è¿œå°äº Wã€‚
- **ä¼˜ç¼ºç‚¹**ï¼š
  - ä¼˜ç‚¹ï¼šé€‚ç”¨äºå·ç§¯å±‚å’Œå…¨è¿æ¥å±‚ï¼Œå‹ç¼©ç‡é«˜ã€‚
  - ç¼ºç‚¹ï¼šè®¡ç®—åˆ†è§£è¿‡ç¨‹å¤æ‚ï¼Œå¯èƒ½éœ€è¦é‡è®­ç»ƒã€‚
- **å·¥å…·ä¸æ¡†æ¶**ï¼šScikit-learn çš„ SVD æ¨¡å—å¯è¾…åŠ©å®ç°ã€‚

åœ¨ Transformer æ¨¡å‹ä¸­ï¼Œä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰æ˜¯ä¸€ç§æµè¡Œå˜ä½“ï¼Œç”¨äºå¾®è°ƒå¤§æ¨¡å‹è€Œæ— éœ€å…¨å‚æ•°æ›´æ–°ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ª PyTorch ä»£ç ç¤ºä¾‹ï¼Œä½¿ç”¨ SVD è¿›è¡Œä½ç§©åˆ†è§£ï¼š

```python
import torch
from torch.linalg import svd

# å‡è®¾ä¸€ä¸ªæƒé‡çŸ©é˜µ
weight = torch.randn(100, 200)

# SVD åˆ†è§£ï¼Œä¿ç•™å‰ k ä¸ªå¥‡å¼‚å€¼
U, S, Vh = svd(weight)
k = 50  # ä½ç§©è¿‘ä¼¼
low_rank = U[:, :k] @ torch.diag(S[:k]) @ Vh[:k, :]

# æŸ¥çœ‹å½¢çŠ¶
print(low_rank.shape)  # (100, 200)ï¼Œä½†å‚æ•°å‡å°‘
```


## ä¸»æµæ¨¡å‹å‹ç¼©å·¥å…·ä¸æ¡†æ¶

ç°ä»£æ¨¡å‹å‹ç¼©å·²ç»å‘å±•å‡ºå®Œæ•´çš„å·¥å…·ç”Ÿæ€ç³»ç»Ÿï¼Œä»¥ä¸‹æ˜¯æœ€ä¸»æµçš„å·¥å…·ï¼š

### ğŸ”§ ç»¼åˆå‹ç¼©æ¡†æ¶
- **Intel Neural Compressor**: æ”¯æŒPyTorch/TensorFlowçš„ç»Ÿä¸€å‹ç¼©åº“ï¼Œæ”¯æŒé‡åŒ–ã€å‰ªæã€è’¸é¦
- **NVIDIA ModelOpt**: æ”¯æŒTensorRTä¼˜åŒ–çš„ç»Ÿä¸€ä¼˜åŒ–æ¡†æ¶
- **ONNX Runtime**: è·¨å¹³å°æ¨ç†ä¼˜åŒ–å¼•æ“ï¼Œæ”¯æŒINT8/FP16é‡åŒ–


### âœ‚ï¸ å‰ªæä¸“ç”¨å·¥å…·
- **PyTorch Pruning**: å†…ç½®çš„`torch.nn.utils.prune`æ¨¡å—
- **NNI (Neural Network Intelligence)**: å¾®è½¯çš„è‡ªåŠ¨åŒ–å‹ç¼©å·¥å…·
- **Torch-Pruning**: é«˜çº§ç»“æ„åŒ–å‰ªæåº“
- **LLM-Pruner**: é€å±‚å­¦ä¹ å¹…åº¦å‰ªæ
- **SparseLLM**: å…¨å±€å‰ªæ
- **Wanda**: åŸºäºæƒé‡å’Œæ¿€æ´»çš„å‰ªææ–¹æ³•


### âš–ï¸ é‡åŒ–å·¥å…·
- **PyTorch Quantization**: å®˜æ–¹é‡åŒ–æ”¯æŒ
- **QNNPACK**: Facebookçš„ç§»åŠ¨ç«¯é‡åŒ–åº“
- **GPTQ**: å¤§æ¨¡å‹é‡åŒ–ä¸“ç”¨å·¥å…·
- **llama.cpp**: å…¶æœ¬èº«æ˜¯ä¸€ä¸ªæ¨ç†æ¡†æ¶ï¼Œä½†æ˜¯æ”¯æŒé‡åŒ–ä¸å‰ªæ
- **SqueezeLLM**: éå‡åŒ€é‡åŒ–+ç¨€ç–åˆ†è§£ï¼Œ3ä½é‡åŒ–è¡¨ç°çªå‡º
- **TensorRT**: NVIDIAçš„æ¨ç†ä¼˜åŒ–å¼•æ“ï¼Œæ”¯æŒINT8/FP16é‡åŒ–


### ğŸ“ çŸ¥è¯†è’¸é¦æ¡†æ¶
- **Hugging Face Distillation**: Transformersåº“çš„è’¸é¦æ”¯æŒ
- **FastDistill**: ä¸“ä¸ºå¤§æ¨¡å‹è®¾è®¡ï¼Œæ”¯æŒå¤šæ•™å¸ˆè’¸é¦
- **MobileBERT**: ç§»åŠ¨ç«¯ä¼˜åŒ–è’¸é¦
- **Distiller**: åŸºäºPyTorchçš„è’¸é¦æ¡†æ¶ï¼Œæ”¯æŒå¤šç§è’¸é¦æ–¹æ³•å’Œæ¨¡å‹è½¬æ¢

## é«˜çº§ä»£ç ç¤ºä¾‹ä¸æœ€ä½³å®è·µ

###  ç»“æ„åŒ–å‰ªæ vs éç»“æ„åŒ–å‰ªæå®Œæ•´å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
from torch.ao.pruning import SaliencyPruner

class ResNetBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.shortcut = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()

    def forward(self, x):
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        return torch.relu(out)

# ç»“æ„åŒ–å‰ªæå®ç°
class StructuredPruner:
    def __init__(self, model, pruning_ratio=0.3):
        self.model = model
        self.pruning_ratio = pruning_ratio
    
    def channel_pruning(self, layer, importance_metric='l1'):
        """é€šé“å‰ªæï¼šç§»é™¤ä¸é‡è¦çš„é€šé“"""
        weight = layer.weight.data
        if importance_metric == 'l1':
            importance = torch.sum(torch.abs(weight), dim=[2, 3])
        elif importance_metric == 'l2':
            importance = torch.sum(weight ** 2, dim=[2, 3]) ** 0.5
        
        # è®¡ç®—è¦ä¿ç•™çš„é€šé“æ•°
        num_channels = weight.shape[0]
        num_keep = int(num_channels * (1 - self.pruning_ratio))
        
        # é€‰æ‹©æœ€é‡è¦çš„é€šé“
        _, top_indices = torch.topk(importance.sum(dim=1), num_keep)
        
        # åˆ›å»ºæ–°çš„æƒé‡
        new_weight = weight[top_indices]
        layer.weight = nn.Parameter(new_weight)
        layer.out_channels = num_keep
        
        return top_indices

# éç»“æ„åŒ–å‰ªæå¯¹æ¯”
class UnstructuredPruner:
    def __init__(self, model, pruning_ratio=0.5):
        self.model = model
        self.pruning_ratio = pruning_ratio
    
    def l1_pruning(self):
        """L1éç»“æ„åŒ–å‰ªæ"""
        for module in self.model.modules():
            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):
                prune.l1_unstructured(module, name='weight', amount=self.pruning_ratio)
        return self.model

# ä½¿ç”¨ç¤ºä¾‹
model = ResNetBlock(64, 128)
pruner = StructuredPruner(model)
pruned_indices = pruner.channel_pruning(model.conv1)
print(f"å‰ªæåé€šé“æ•°: {len(pruned_indices)}")
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
- **ç»“æ„åŒ–å‰ªæ**ï¼šå‚æ•°å‡å°‘40-60%ï¼Œé€Ÿåº¦æå‡2-4xï¼Œç¡¬ä»¶å‹å¥½
- **éç»“æ„åŒ–å‰ªæ**ï¼šå‚æ•°å‡å°‘90%ï¼Œä½†ç¨€ç–çŸ©é˜µè®¡ç®—æ•ˆç‡ä½
- **æ··åˆå‰ªæ**ï¼šå…ˆç»“æ„åŒ–åéç»“æ„åŒ–ï¼Œå¹³è¡¡å‹ç¼©ç‡å’Œæ€§èƒ½

**æœ€ä½³å®è·µ**ï¼š
```python

import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
from typing import Dict, List, Tuple, Callable, Optional
import numpy as np
from copy import deepcopy

class GradualPruningScheduler:
    """
    æ¸è¿›å¼å‰ªæè°ƒåº¦å™¨ï¼Œå®ç°æ›´ç²¾ç»†çš„å‰ªææ§åˆ¶
    """
    
    def __init__(self, model: nn.Module, 
                 pruning_method: str = "l1_unstructured",
                 accuracy_threshold: float = 0.02,
                 max_pruning_ratio: float = 0.8,
                 patience: int = 3):
        self.model = model
        self.pruning_method = pruning_method
        self.accuracy_threshold = accuracy_threshold  # æœ€å¤§å…è®¸çš„ç²¾åº¦æŸå¤±
        self.max_pruning_ratio = max_pruning_ratio    # å•å±‚æœ€å¤§å‰ªææ¯”ä¾‹
        self.patience = patience                      # æ—©åœè€å¿ƒå€¼
        
        # å­˜å‚¨æ¯å±‚çš„æœ€ä½³å‰ªææ¯”ä¾‹
        self.layer_best_ratios = {}
        self.original_state = deepcopy(model.state_dict())
        
    def evaluate_model(self, model: nn.Module, 
                      eval_loader: torch.utils.data.DataLoader,
                      criterion: Callable) -> float:
        """è¯„ä¼°æ¨¡å‹ç²¾åº¦"""
        model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in eval_loader:
                output = model(data)
                total_loss += criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        accuracy = correct / total
        return accuracy, total_loss / len(eval_loader)
    
    def find_optimal_pruning_ratio(self, 
                                 layer: nn.Module,
                                 layer_name: str,
                                 eval_loader: torch.utils.data.DataLoader,
                                 criterion: Callable,
                                 original_accuracy: float) -> float:
        """ä¸ºå•å±‚å¯»æ‰¾æœ€ä¼˜å‰ªææ¯”ä¾‹"""
        best_ratio = 0.0
        best_accuracy = original_accuracy
        
        # æµ‹è¯•ä¸åŒçš„å‰ªææ¯”ä¾‹
        prune_ratios = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]
        
        for ratio in prune_ratios:
            if ratio > self.max_pruning_ratio:
                continue
                
            # åˆ›å»ºä¸´æ—¶æ¨¡å‹è¿›è¡Œæµ‹è¯•
            temp_model = deepcopy(self.model)
            temp_layer = self._get_layer_by_name(temp_model, layer_name)
            
            # åº”ç”¨å‰ªæ
            self._apply_pruning(temp_layer, ratio, self.pruning_method)
            
            # è¯„ä¼°ç²¾åº¦
            accuracy, loss = self.evaluate_model(temp_model, eval_loader, criterion)
            accuracy_drop = original_accuracy - accuracy
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç²¾åº¦è¦æ±‚
            if accuracy_drop <= self.accuracy_threshold and accuracy >= best_accuracy:
                best_ratio = ratio
                best_accuracy = accuracy
                
            # æ¸…ç†ä¸´æ—¶æ¨¡å‹
            del temp_model
            
        return best_ratio
    
    def gradual_compression(self,
                          target_accuracy: float,
                          eval_loader: torch.utils.data.DataLoader,
                          criterion: Callable,
                          fine_tune_fn: Callable,
                          max_iterations: int = 10) -> nn.Module:
        """
        æ¸è¿›å¼å‹ç¼©ä¸»å‡½æ•°
        
        Args:
            target_accuracy: ç›®æ ‡ç²¾åº¦
            eval_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            criterion: æŸå¤±å‡½æ•°
            fine_tune_fn: å¾®è°ƒå‡½æ•°
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        current_accuracy, _ = self.evaluate_model(self.model, eval_loader, criterion)
        iterations = 0
        no_improvement_count = 0
        best_accuracy = current_accuracy
        
        print(f"åˆå§‹ç²¾åº¦: {current_accuracy:.4f}")
        
        while current_accuracy > target_accuracy and iterations < max_iterations:
            print(f"\n=== è¿­ä»£ {iterations + 1} ===")
            
            # ä¿å­˜å½“å‰æœ€ä½³çŠ¶æ€
            best_state = deepcopy(self.model.state_dict())
            
            # é€å±‚å¯»æ‰¾æœ€ä¼˜å‰ªææ¯”ä¾‹å¹¶åº”ç”¨
            total_pruned = self._prune_layers_gradually(eval_loader, criterion, current_accuracy)
            
            if total_pruned == 0:
                print("æ— æ³•æ‰¾åˆ°æ»¡è¶³ç²¾åº¦è¦æ±‚çš„å‰ªææ¯”ä¾‹ï¼Œåœæ­¢å‰ªæ")
                break
                
            # å¾®è°ƒæ¢å¤ç²¾åº¦
            print("å¼€å§‹å¾®è°ƒ...")
            fine_tune_fn(self.model)
            
            # è¯„ä¼°å¾®è°ƒåç²¾åº¦
            new_accuracy, _ = self.evaluate_model(self.model, eval_loader, criterion)
            accuracy_drop = current_accuracy - new_accuracy
            
            print(f"å‰ªæåç²¾åº¦: {new_accuracy:.4f}, ç²¾åº¦ä¸‹é™: {accuracy_drop:.4f}")
            
            if new_accuracy >= best_accuracy - self.accuracy_threshold:
                if new_accuracy > best_accuracy:
                    best_accuracy = new_accuracy
                    best_state = deepcopy(self.model.state_dict())
                no_improvement_count = 0
            else:
                no_improvement_count += 1
                # æ¢å¤æœ€ä½³çŠ¶æ€
                self.model.load_state_dict(best_state)
                print(f"ç²¾åº¦ä¸‹é™è¿‡å¤šï¼Œæ¢å¤ä¹‹å‰çŠ¶æ€ (ç¬¬{no_improvement_count}æ¬¡)")
            
            current_accuracy = new_accuracy
            iterations += 1
            
            # æ—©åœæ£€æŸ¥
            if no_improvement_count >= self.patience:
                print("æ—©åœ: ç²¾åº¦è¿ç»­æœªæå‡")
                break
        
        # åº”ç”¨æœ€ç»ˆçš„æœ€ä½³çŠ¶æ€
        self.model.load_state_dict(best_state)
        final_accuracy, _ = self.evaluate_model(self.model, eval_loader, criterion)
        print(f"\næœ€ç»ˆç²¾åº¦: {final_accuracy:.4f}")
        
        return self.model
    
    def _prune_layers_gradually(self,
                              eval_loader: torch.utils.data.DataLoader,
                              criterion: Callable,
                              original_accuracy: float) -> int:
        """é€å±‚å‰ªæ"""
        layers_to_prune = self._get_prunable_layers()
        total_pruned = 0
        
        for layer_name, layer in layers_to_prune:
            print(f"å¤„ç†å±‚: {layer_name}")
            
            # å¯»æ‰¾æœ€ä¼˜å‰ªææ¯”ä¾‹
            best_ratio = self.find_optimal_pruning_ratio(
                layer, layer_name, eval_loader, criterion, original_accuracy)
            
            if best_ratio > 0:
                self._apply_pruning(layer, best_ratio, self.pruning_method)
                self.layer_best_ratios[layer_name] = best_ratio
                total_pruned += 1
                print(f"  åº”ç”¨å‰ªææ¯”ä¾‹: {best_ratio:.2f}")
            else:
                print(f"  æœªæ‰¾åˆ°åˆé€‚çš„å‰ªææ¯”ä¾‹")
        
        return total_pruned
    
    def _get_prunable_layers(self) -> List[Tuple[str, nn.Module]]:
        """è·å–å¯å‰ªæçš„å±‚"""
        prunable_layers = []
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                prunable_layers.append((name, module))
        return prunable_layers
    
    def _get_layer_by_name(self, model: nn.Module, layer_name: str) -> nn.Module:
        """é€šè¿‡åç§°è·å–å±‚"""
        modules = dict(model.named_modules())
        return modules[layer_name]
    
    def _apply_pruning(self, layer: nn.Module, amount: float, method: str):
        """åº”ç”¨å‰ªæ"""
        if method == "l1_unstructured":
            prune.l1_unstructured(layer, name="weight", amount=amount)
        elif method == "random_unstructured":
            prune.random_unstructured(layer, name="weight", amount=amount)
        elif method == "ln_structured":
            prune.ln_structured(layer, name="weight", amount=amount, n=2, dim=0)
        # å¯ä»¥æ·»åŠ å…¶ä»–å‰ªææ–¹æ³•

# ä½¿ç”¨ç¤ºä¾‹
def create_fine_tune_function(optimizer_fn: Callable, 
                            scheduler_fn: Callable,
                            train_loader: torch.utils.data.DataLoader,
                            epochs: int = 5) -> Callable:
    """åˆ›å»ºå¾®è°ƒå‡½æ•°"""
    def fine_tune(model: nn.Module):
        model.train()
        optimizer = optimizer_fn(model.parameters())
        scheduler = scheduler_fn(optimizer)
        criterion = nn.CrossEntropyLoss()
        
        for epoch in range(epochs):
            total_loss = 0
            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                
                # å¯¹äºå‰ªæåçš„æ¨¡å‹ï¼Œéœ€è¦ç¡®ä¿æ¢¯åº¦ä¸ä¼šæ›´æ–°è¢«å‰ªæçš„æƒé‡
                optimizer.step()
                total_loss += loss.item()
            
            scheduler.step()
            if epoch % 2 == 0:
                print(f"å¾®è°ƒ Epoch {epoch}, æŸå¤±: {total_loss/len(train_loader):.4f}")
    
    return fine_tune

# ç®€åŒ–çš„ä½¿ç”¨ç¤ºä¾‹
def simple_gradual_pruning(model: nn.Module,
                         target_accuracy: float,
                         eval_loader: torch.utils.data.DataLoader,
                         train_loader: torch.utils.data.DataLoader,
                         max_iterations: int = 5) -> nn.Module:
    """
    ç®€åŒ–ç‰ˆçš„æ¸è¿›å¼å‰ªæå‡½æ•°
    """
    criterion = nn.CrossEntropyLoss()
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    def create_optimizer(params):
        return torch.optim.Adam(params, lr=1e-4, weight_decay=1e-4)
    
    def create_scheduler(optimizer):
        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)
    
    fine_tune_fn = create_fine_tune_function(
        create_optimizer, create_scheduler, train_loader, epochs=5)
    
    # åˆ›å»ºå‰ªæè°ƒåº¦å™¨
    pruner = GradualPruningScheduler(
        model,
        pruning_method="l1_unstructured",
        accuracy_threshold=0.02,  # 2%çš„ç²¾åº¦æŸå¤±å®¹å¿åº¦
        max_pruning_ratio=0.7,    # å•å±‚æœ€å¤§å‰ªæ70%
        patience=2
    )
    
    # æ‰§è¡Œå‰ªæ
    compressed_model = pruner.gradual_compression(
        target_accuracy=target_accuracy,
        eval_loader=eval_loader,
        criterion=criterion,
        fine_tune_fn=fine_tune_fn,
        max_iterations=max_iterations
    )
    
    return compressed_model

### ä½¿ç”¨ç¤ºä¾‹

# å‡è®¾ä½ æœ‰ä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨
model = YourTrainedModel()
train_loader = YourTrainDataLoader()
eval_loader = YourEvalDataLoader()

# åº”ç”¨æ¸è¿›å¼å‰ªæ
compressed_model = simple_gradual_pruning(
    model=model,
    target_accuracy=0.85,  # ç›®æ ‡ç²¾åº¦85%
    eval_loader=eval_loader,
    train_loader=train_loader,
    max_iterations=5
)

# ä¿å­˜å‰ªæåçš„æ¨¡å‹
torch.save(compressed_model.state_dict(), "pruned_model.pth")
```

### ä¸‰ç§é‡åŒ–æ–¹å¼å®Œæ•´å®ç°

```python
import torch
from torch.ao.quantization import quantize_dynamic, prepare_qat, convert
from torch.ao.quantization import get_default_qconfig

# åŠ¨æ€é‡åŒ– - é€‚ç”¨äºRNN/LSTM
def apply_dynamic_quantization(model):
    """åŠ¨æ€é‡åŒ–ï¼šè¿è¡Œæ—¶é‡åŒ–æ¿€æ´»"""
    quantized_model = quantize_dynamic(
        model,
        {nn.Linear, nn.LSTM, nn.GRU, nn.Conv2d},
        dtype=torch.qint8
    )
    return quantized_model

# é™æ€é‡åŒ– - é€‚ç”¨äºCNN
def apply_static_quantization(model, calibration_data):
    """é™æ€é‡åŒ–ï¼šé¢„å…ˆé‡åŒ–æƒé‡å’Œæ¿€æ´»"""
    model.eval()
    model.qconfig = get_default_qconfig('fbgemm')
    
    prepared_model = torch.quantization.prepare(model)
    
    # æ ¡å‡† - ä½¿ç”¨ä»£è¡¨æ€§æ•°æ®
    with torch.no_grad():
        for data, _ in calibration_data:
            prepared_model(data)
    
    quantized_model = torch.quantization.convert(prepared_model)
    return quantized_model

# é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ(QAT) - æœ€ä½³æ•ˆæœ
class QATTrainer:
    def __init__(self, model):
        self.model = model
        self.setup_qat()
    
    def setup_qat(self):
        """è®¾ç½®QATé…ç½®"""
        self.model.train()
        self.model.qconfig = get_default_qconfig('fbgemm')
        self.model = torch.ao.quantization.prepare_qat(self.model)
    
    def train_with_qat(self, train_loader, epochs=5, lr=0.01):
        """QATè®­ç»ƒ"""
        optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)
        criterion = nn.CrossEntropyLoss()
        
        for epoch in range(epochs):
            for data, target in train_loader:
                optimizer.zero_grad()
                output = self.model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
        
        return torch.quantization.convert(self.model.eval())

# ä½¿ç”¨ç¤ºä¾‹
class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, 3)
        self.conv2 = nn.Conv2d(64, 128, 3)
        self.fc = nn.Linear(128 * 6 * 6, 10)
    
    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.conv2(x))
        x = torch.max_pool2d(x, 2)
        x = x.view(-1, 128 * 6 * 6)
        x = self.fc(x)
        return x

model = SimpleCNN()
```

#### åŸºç¡€é‡åŒ–å‘½ä»¤
```

# å°†HFæ ¼å¼æ¨¡å‹è½¬æ¢ä¸ºGGUFæ ¼å¼
python convert_hf_to_gguf.py ./models/your-model/ --outfile model-f16.gguf

# æ‰§è¡Œ4-bité‡åŒ–
./llama-quantize model-f16.gguf model-q4km.gguf Q4_K_M

# ä½¿ç”¨é‡è¦æ€§çŸ©é˜µä¼˜åŒ–é‡åŒ–
./llama-imatrix -m model-f16.gguf -f calibration-data.txt -o imatrix.dat
./llama-quantize --imatrix imatrix.dat model-f16.gguf model-optimized.gguf Q4_K_M
```
#### æ¸è¿›å¼å‰ªæä¼˜åŒ–
```
# ç¬¬ä¸€é˜¶æ®µï¼šè½»åº¦é‡åŒ–
./quantize model.f32.gguf model.q8_0.gguf Q8_0

# ç¬¬äºŒé˜¶æ®µï¼šä¸­åº¦é‡åŒ–+è½»åº¦å‰ªæ
./quantize --prune-layers 10,15 model.q8_0.gguf model.q4_0.gguf Q4_0

# ç¬¬ä¸‰é˜¶æ®µï¼šæ·±åº¦ä¼˜åŒ–
./quantize --prune-layers 5,10,15,20 --imatrix imatrix.gguf model.q4_0.gguf final.gguf Q4_0

```


**é‡åŒ–æ€§èƒ½å¯¹æ¯”è¡¨**ï¼š

| é‡åŒ–ç±»å‹ | å‚æ•°å‡å°‘ | æ¨ç†é€Ÿåº¦æå‡ | å‡†ç¡®ç‡æŸå¤± | é€‚ç”¨åœºæ™¯ |
|---------|----------|-------------|-----------|----------|
| åŠ¨æ€é‡åŒ– | 75% | 3-4x | <1% | RNN/LSTM |
| é™æ€é‡åŒ– | 75% | 4x | <2% | CNNæ¨¡å‹ |
| QATé‡åŒ– | 75% | 3x | <0.5% | é«˜ç²¾åº¦è¦æ±‚ |
| INT4é‡åŒ– | 87.5% | 8x | 2-3% | æè‡´å‹ç¼© |

### é«˜çº§çŸ¥è¯†è’¸é¦æŠ€æœ¯

```python
import torch.nn.functional as F
from transformers import AutoModel, AutoTokenizer

class AdvancedDistiller:
    def __init__(self, teacher_model, student_model, temperature=4.0):
        self.teacher = teacher_model
        self.student = student_model
        self.temperature = temperature
        
    def attention_distillation(self, teacher_attn, student_attn):
        """æ³¨æ„åŠ›è’¸é¦ï¼šåŒ¹é…æ³¨æ„åŠ›æƒé‡"""
        loss = 0
        for t_attn, s_attn in zip(teacher_attn, student_attn):
            t_probs = F.softmax(t_attn / self.temperature, dim=-1)
            s_probs = F.log_softmax(s_attn / self.temperature, dim=-1)
            loss += F.kl_div(s_probs, t_probs, reduction='batchmean')
        return loss
    
    def hidden_state_distillation(self, teacher_hidden, student_hidden):
        """éšè—çŠ¶æ€è’¸é¦ï¼šåŒ¹é…ä¸­é—´å±‚è¡¨ç¤º"""
        loss = 0
        for t_hidden, s_hidden in zip(teacher_hidden, student_hidden):
            loss += F.mse_loss(s_hidden, t_hidden)
        return loss
    
    def multi_teacher_distillation(self, outputs, teacher_outputs):
        """å¤šæ•™å¸ˆè’¸é¦ï¼šé›†æˆå¤šä¸ªæ•™å¸ˆ"""
        teacher_logits = torch.stack([t['logits'] for t in teacher_outputs])
        avg_teacher = torch.mean(teacher_logits, dim=0)
        
        soft_targets = F.softmax(avg_teacher / self.temperature, dim=-1)
        soft_outputs = F.log_softmax(outputs / self.temperature, dim=-1)
        distill_loss = F.kl_div(soft_outputs, soft_targets, reduction='batchmean')
        
        return distill_loss

# TinyBERTè’¸é¦å®Œæ•´å®ç°
class TinyBERTDistillation:
    def __init__(self, teacher_name="bert-base-uncased"):
        self.teacher = AutoModel.from_pretrained(teacher_name)
        self.student = AutoModel.from_pretrained("huawei-noah/TinyBERT_6L_768D")
        self.tokenizer = AutoTokenizer.from_pretrained(teacher_name)
    
    def distill_step(self, texts, labels):
        """å•æ­¥è’¸é¦è®­ç»ƒ"""
        inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
        
        # æ•™å¸ˆæ¨¡å‹è¾“å‡ºï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
        with torch.no_grad():
            teacher_outputs = self.teacher(**inputs)
        
        # å­¦ç”Ÿæ¨¡å‹è¾“å‡º
        student_outputs = self.student(**inputs)
        
        # è®¡ç®—è’¸é¦æŸå¤±
        distill_loss = self.attention_distillation(
            teacher_outputs.attentions, 
            student_outputs.attentions
        )
        
        return distill_loss

# ä½¿ç”¨ç¤ºä¾‹
distiller = TinyBERTDistillation()
```

### LoRA/AdaLoRA/QLoRA å®Œæ•´å®ç°

```python
from peft import LoraConfig, get_peft_model, AdaLoraConfig
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

class LoRATrainer:
    def __init__(self, model_name="microsoft/DialoGPT-medium"):
        self.model_name = model_name
    
    def setup_lora(self, rank=8, alpha=32):
        """æ ‡å‡†LoRAé…ç½®"""
        lora_config = LoraConfig(
            r=rank,
            lora_alpha=alpha,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            lora_dropout=0.1,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        model = AutoModelForCausalLM.from_pretrained(self.model_name)
        lora_model = get_peft_model(model, lora_config)
        return lora_model
    
    def setup_qlora(self, load_in_4bit=True, r=64):
        """QLoRAé‡åŒ–é…ç½®"""
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=load_in_4bit,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=quantization_config
        )
        
        qlora_config = LoraConfig(
            r=r,
            lora_alpha=16,
            target_modules=["q_proj", "k_proj", "v_proj"],
            lora_dropout=0.05
        )
        
        return get_peft_model(model, qlora_config)

# AdaLoRAé…ç½®
class AdaLoRATrainer:
    def __init__(self, model_name):
        self.model_name = model_name
    
    def setup_adalora(self, initial_rank=16, target_rank=8):
        """AdaLoRAè‡ªé€‚åº”ç§©é…ç½®"""
        adalora_config = AdaLoraConfig(
            r=initial_rank,
            lora_alpha=32,
            target_r=target_rank,
            init_r=initial_rank,
            tinit=200,
            tfinal=1000,
            deltaT=10,
            orth_reg_weight=0.5,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
        )
        
        model = AutoModelForCausalLM.from_pretrained(self.model_name)
        return get_peft_model(model, adalora_config)

# æ€§èƒ½å¯¹æ¯”
lora_performance = {
    "LoRA": {"å‚æ•°å‡å°‘": "99%", "å†…å­˜å‡å°‘": "95%", "å‡†ç¡®ç‡": "ä¿æŒ99%+"},
    "QLoRA": {"å‚æ•°å‡å°‘": "99.5%", "å†…å­˜å‡å°‘": "97%", "æ¨ç†é€Ÿåº¦": "æå‡3x"},
    "AdaLoRA": {"å‚æ•°å‡å°‘": "99%", "å†…å­˜å‡å°‘": "96%", "è‡ªé€‚åº”": "åŠ¨æ€è°ƒæ•´ç§©"}
}
```

## æ··åˆå‹ç¼©ç­–ç•¥ä¸å®æˆ˜éƒ¨ç½²

### ç»„åˆå‹ç¼©ç­–ç•¥

```python
class HybridCompressor:
    def __init__(self, model):
        self.model = model
    
    def pruning_then_quantization(self, pruning_ratio=0.5):
        """å…ˆå‰ªæåé‡åŒ–"""
        # æ­¥éª¤1: ç»“æ„åŒ–å‰ªæ
        pruned_model = self.apply_structured_pruning(pruning_ratio)
        
        # æ­¥éª¤2: é‡åŒ–
        quantized_model = self.apply_static_quantization(pruned_model)
        
        return quantized_model
    
    def quantization_then_distillation(self, teacher_model):
        """é‡åŒ–+è’¸é¦ç»„åˆ"""
        quantized_teacher = self.quantize_model(teacher_model)
        distilled_model = self.distill_to_student(quantized_teacher)
        return distilled_model

# å®æˆ˜æ€§èƒ½æ•°æ®
hybrid_performance = {
    "ResNet50 + å‰ªæ50% + é‡åŒ–": {
        "å‚æ•°å‡å°‘": "90%",
        "æ¨ç†é€Ÿåº¦æå‡": "10x",
        "å‡†ç¡®ç‡æŸå¤±": "2%",
        "å†…å­˜å ç”¨": "å‡å°‘85%"
    },
    "BERT + LoRA + é‡åŒ–": {
        "å‚æ•°å‡å°‘": "99.5%",
        "æ¨ç†é€Ÿåº¦æå‡": "3x",
        "å‡†ç¡®ç‡æŸå¤±": "1%"
    }
}
```

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ä¼˜åŒ–

```python
# TensorRTä¼˜åŒ–
import tensorrt as trt

def optimize_for_tensorrt(model, input_shape):
    """TensorRTä¼˜åŒ–æ¨¡å‹éƒ¨ç½²"""
    model.eval()
    dummy_input = torch.randn(input_shape)
    
    # å¯¼å‡ºONNX
    torch.onnx.export(model, dummy_input, "model.onnx", 
                     export_params=True, opset_version=11)
    
    # TensorRTä¼˜åŒ–æµç¨‹
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)
    network = builder.create_network()
    parser = trt.OnnxParser(network, logger)
    
    with open("model.onnx", "rb") as model_file:
        parser.parse(model_file.read())
    
    # é…ç½®ä¼˜åŒ–å‚æ•°
    config = builder.create_builder_config()
    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # 1GB
    
    # ç”Ÿæˆä¼˜åŒ–å¼•æ“
    engine = builder.build_engine(network, config)
    return engine

# ONNX Runtimeä¼˜åŒ–
from onnxruntime.quantization import quantize_dynamic, QuantType

def optimize_for_onnx(model_path):
    """ONNX Runtimeä¼˜åŒ–"""
    quantized_model = quantize_dynamic(
        model_path,
        model_path.replace('.onnx', '_quantized.onnx'),
        weight_type=QuantType.QInt8
    )
    return quantized_model

# Dockeréƒ¨ç½²ç¤ºä¾‹
'''
FROM pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY model_compressed.pt .
COPY inference.py .
CMD ["python", "inference.py"]
'''
```

## æœ€ç»ˆæ€§èƒ½å¯¹æ¯”è¡¨

| å‹ç¼©æ–¹æ³• | å‚æ•°å‡å°‘ | æ¨ç†é€Ÿåº¦æå‡ | å‡†ç¡®ç‡æŸå¤± | é€‚ç”¨åœºæ™¯ | å·¥å…·æ¨è |
|---------|----------|-------------|-----------|----------|----------|
| ç»“æ„åŒ–å‰ªæ | 40-90% | 2-10x | <2% | CNNæ¨¡å‹ | PyTorch Pruning |
| åŠ¨æ€é‡åŒ– | 75% | 3-4x | <1% | RNN/LSTM | torch.quantization |
| é™æ€é‡åŒ– | 75% | 4x | <2% | CNNæ¨¡å‹ | Intel Neural Compressor |
| QATé‡åŒ– | 75% | 3x | <0.5% | é«˜ç²¾åº¦è¦æ±‚ | PyTorch QAT |
| LoRAå¾®è°ƒ | 99% | - | <1% | å¤§æ¨¡å‹å¾®è°ƒ | PEFTåº“ |
| çŸ¥è¯†è’¸é¦ | 40-60% | 2-3x | <3% | æ¨¡å‹å‹ç¼© | Hugging Face |
| INT4é‡åŒ– | 87.5% | 8x | 2-3% | æè‡´å‹ç¼© | GPTQ |
| æ··åˆç­–ç•¥ | 90%+ | 10x+ | <3% | æè‡´ä¼˜åŒ– | ç»„åˆæ–¹æ³• |

## æœ€ä½³å®è·µ

### ğŸ“Š é€‰æ‹©å†³ç­–æ ‘

```
æ¨¡å‹ç±»å‹?
â”œâ”€â”€ LLM (GPT/BERT)
â”‚   â”œâ”€â”€ å¾®è°ƒéœ€æ±‚ â†’ LoRA/QLoRA
â”‚   â””â”€â”€ æ¨ç†ä¼˜åŒ– â†’ INT8é‡åŒ– + çŸ¥è¯†è’¸é¦
â”œâ”€â”€ CNN (ResNet/EfficientNet)
â”‚   â”œâ”€â”€ è¾¹ç¼˜éƒ¨ç½² â†’ ç»“æ„åŒ–å‰ªæ + é™æ€é‡åŒ–
â”‚   â””â”€â”€ äº‘ç«¯éƒ¨ç½² â†’ åŠ¨æ€é‡åŒ– + TensorRT
â””â”€â”€ RNN/LSTM
    â”œâ”€â”€ ç§»åŠ¨ç«¯ â†’ åŠ¨æ€é‡åŒ–
    â””â”€â”€ æœåŠ¡å™¨ â†’ é™æ€é‡åŒ– + å‰ªæ
```
#### æè‡´å‹ç¼©
è’¸é¦->sft->å‰ªæ->é‡åŒ–

## æ¨¡å‹å‹ç¼©çš„å®é™…åº”ç”¨ä¸æŒ‘æˆ˜

æ¨¡å‹å‹ç¼©å·²åœ¨å¤šä¸ªé¢†åŸŸè½åœ°ï¼š
- **ç§»åŠ¨ AI**ï¼šå¦‚ Siri æˆ– Google Assistantï¼Œä½¿ç”¨é‡åŒ–æ¨¡å‹å®ç°æœ¬åœ°è¯­éŸ³è¯†åˆ«ã€‚
- **è¾¹ç¼˜è®¡ç®—**ï¼šè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­ï¼Œå‹ç¼© YOLO æ¨¡å‹ç”¨äºå®æ—¶ç‰©ä½“æ£€æµ‹ã€‚
- **äº‘æœåŠ¡**ï¼šAWS æˆ– Azure é€šè¿‡å‹ç¼©é™ä½æ¨ç†æˆæœ¬ã€‚
- **å¼€æºç¤¾åŒº**ï¼šHugging Face çš„ Model Hub æä¾›å¤§é‡å‹ç¼©æ¨¡å‹ï¼Œå¦‚ TinyBERTã€‚

ç„¶è€Œï¼ŒæŒ‘æˆ˜çŠ¹å­˜ï¼š
- **æ€§èƒ½æƒè¡¡**ï¼šè¿‡åº¦å‹ç¼©å¯èƒ½å¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚
- **ç¡¬ä»¶å…¼å®¹**ï¼šä¸åŒè®¾å¤‡å¯¹å‹ç¼©æ–¹æ³•çš„æ”¯æŒåº¦ä¸åŒã€‚
- **å®‰å…¨æ€§**ï¼šå‹ç¼©æ¨¡å‹å¯èƒ½æ›´å®¹æ˜“è¢«æ”»å‡»ï¼Œå¦‚æ¨¡å‹çªƒå–ã€‚


## ç»“è®º

æ¨¡å‹å‹ç¼©æ˜¯ AI èµ°å‘æ™®æƒ çš„å…³é”®æŠ€æœ¯ï¼Œå®ƒä¸ä»…è§£å†³äº†èµ„æºç“¶é¢ˆï¼Œè¿˜æ¨åŠ¨äº†ç»¿è‰²è®¡ç®—çš„å‘å±•ã€‚æœªæ¥ï¼Œéšç€ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰å’Œç¡¬ä»¶ååŒè®¾è®¡çš„è¿›æ­¥ï¼Œå‹ç¼©æŠ€æœ¯å°†æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆã€‚å¦‚æœä½ æ­£ä»äº‹ AI é¡¹ç›®ï¼Œä¸å¦¨ä»ç®€å•çš„æ–¹æ³•å…¥æ‰‹ï¼Œå°è¯•å‹ç¼©ä½ çš„æ¨¡å‹â€”â€”æˆ–è®¸ä¼šå¸¦æ¥æƒŠå–œï¼
