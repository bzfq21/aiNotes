# GRPO (Group Relative Policy Optimization) ç®—æ³•

## 1. èƒŒæ™¯ä¸åŠ¨æœº

GRPO æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ PPOçš„ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œç”±deepseekæå‡ºã€‚å®ƒé€šè¿‡ç»„å†…ç›¸å¯¹å¥–åŠ±ä¿¡å·æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œé¿å…äº†ä¼ ç»ŸPPOä¸­éœ€è¦å•ç‹¬ä»·å€¼å‡½æ•°ä¼°è®¡çš„é—®é¢˜ã€‚GRPOå·²æˆä¸ºæ¨ç†æ¨¡å‹çš„é¦–é€‰ã€‚

### 1.1 æ ¸å¿ƒæ€æƒ³
- **ç»„ç›¸å¯¹æ€§**: ä½¿ç”¨ç»„å†…æ ·æœ¬çš„å¥–åŠ±å‡å€¼ï¼Œä½œä¸ºè¯„ä»·å•ä¸ªåŠ¨ä½œå¥½åçš„åŸºå‡†ï¼Œé€šè¿‡ç»„å†…ç›¸å¯¹æ€§èƒ½æ¥æŒ‡å¯¼ç­–ç•¥æ›´æ–°
- **æ— éœ€ä»·å€¼å‡½æ•°**: ç›´æ¥åˆ©ç”¨ç»„å†…æ ·æœ¬çš„ç›¸å¯¹å¥–åŠ±è¿›è¡Œä¼˜åŒ–
- **ç¨³å®šæ€§**: é€šè¿‡clipæœºåˆ¶é™åˆ¶ç­–ç•¥æ›´æ–°çš„å¹…åº¦

## 2. æ•°å­¦åŸºç¡€

### 2.1 é—®é¢˜å®šä¹‰

ç»™å®šï¼š
- çŠ¶æ€ç©ºé—´ $\mathcal{S}$
- åŠ¨ä½œç©ºé—´ $\mathcal{A}$  
- ç­–ç•¥å‡½æ•° $\pi_\theta(a|s)$ å‚æ•°åŒ–ä¸º $\theta$
- å¥–åŠ±å‡½æ•° $r(s,a)$
- ä¸€ç»„æ ·æœ¬ $\{s_i, a_i, r_i\}_{i=1}^G$ï¼Œå…¶ä¸­ $G$ ä¸ºç»„å¤§å°

### 2.2 ç»„å½’ä¸€åŒ–å¥–åŠ±

å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œè®¡ç®—ç›¸å¯¹å¥–åŠ±ï¼š

$$\hat{r}_i = \frac{r_i - \mu_r}{\sigma_r + \epsilon}$$

å…¶ä¸­ï¼š
- $\mu_r = \frac{1}{G}\sum_{j=1}^G r_j$ æ˜¯ç»„å†…å¹³å‡å¥–åŠ±
- $\sigma_r = \sqrt{\frac{1}{G}\sum_{j=1}^G (r_j - \mu_r)^2}$ æ˜¯ç»„å†…å¥–åŠ±æ ‡å‡†å·®
- $\epsilon$ æ˜¯æ•°å€¼ç¨³å®šå¸¸æ•°ï¼ˆé€šå¸¸å– $10^{-8}$ï¼‰

### 2.3 ä¼˜åŠ¿å‡½æ•°ä¼°è®¡

GRPO çš„ä¼˜åŠ¿å‡½æ•°ä¼°è®¡ç®€åŒ–ä¸ºå½’ä¸€åŒ–å¥–åŠ±ï¼š

$$A_i^{\text{GRPO}} = \hat{r}_i$$

è¿™ä¸ä¼ ç»ŸPPOä¸­çš„GAEï¼ˆGeneralized Advantage Estimationï¼‰ä¸åŒï¼Œåè€…éœ€è¦ï¼š

$$A_t^{\text{GAE}} = \sum_{l=0}^{\infty}(\gamma\lambda)^l\delta_{t+l}$$

å…¶ä¸­ $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ éœ€è¦ä»·å€¼å‡½æ•° $V$ã€‚

## 3. ç›®æ ‡å‡½æ•°æ¨å¯¼

### 3.1 åŸºæœ¬PPOç›®æ ‡å‡½æ•°

ä¼ ç»ŸPPOçš„ç›®æ ‡å‡½æ•°ï¼š

$$L^{\text{PPO}}(\theta) = \mathbb{E}_t\left[\min\left(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}A_t, \text{clip}\left(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}, 1-\epsilon, 1+\epsilon\right)A_t\right)\right]$$

### 3.2 GRPOç›®æ ‡å‡½æ•°

GRPOå°†PPOç›®æ ‡å‡½æ•°ç®€åŒ–ä¸ºï¼š

$$L^{\text{GRPO}}(\theta) = \mathbb{E}_{i\sim\text{Group}}\left[\min\left(\frac{\pi_\theta(a_i|s_i)}{\pi_{\theta_{\text{old}}}(a_i|s_i)}\hat{r}_i, \text{clip}\left(\frac{\pi_\theta(a_i|s_i)}{\pi_{\theta_{\text{old}}}(a_i|s_i)}, 1-\epsilon, 1+\epsilon\right)\hat{r}_i\right)\right]$$

### 3.3 æ•°å­¦æ¨å¯¼

**æ­¥éª¤1: ç­–ç•¥æ¢¯åº¦å®šç†**

ç­–ç•¥æ¢¯åº¦ï¼š
$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(a|s) A(s,a)\right]$$

**æ­¥éª¤2: é‡è¦æ€§é‡‡æ ·**

ä½¿ç”¨æ—§ç­–ç•¥ $\pi_{\theta_{\text{old}}}$ è¿›è¡Œé‡è¦æ€§é‡‡æ ·ï¼š

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_{\theta_{\text{old}}}}\left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}\nabla_\theta \log \pi_\theta(a|s) A(s,a)\right]$$

**æ­¥éª¤3: Clipæœºåˆ¶**

ä¸ºäº†é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œå¼•å…¥clipï¼š

$$L(\theta) = \mathbb{E}\left[\min(r(\theta)A, \text{clip}(r(\theta), 1-\epsilon, 1+\epsilon)A)\right]$$

å…¶ä¸­ $r(\theta) = \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}$ æ˜¯æ¦‚ç‡æ¯”ã€‚

**æ­¥éª¤4: GRPOç®€åŒ–**

åœ¨GRPOä¸­ï¼Œ$A = \hat{r}_i$ï¼Œå› æ­¤ï¼š

$$L^{\text{GRPO}}(\theta) = \frac{1}{G}\sum_{i=1}^G \min\left(r_i(\theta)\hat{r}_i, \text{clip}(r_i(\theta), 1-\epsilon, 1+\epsilon)\hat{r}_i\right)$$

## 4. ç®—æ³•å®ç°ç»†èŠ‚

### 4.1 å®Œæ•´ç®—æ³•æµç¨‹

```python
# ä¼ªä»£ç 
for iteration in range(num_iterations):
    # æ”¶é›†æ ·æœ¬
    batch = collect_samples(env, policy, batch_size)
    
    # æŒ‰ç»„åˆ’åˆ†æ ·æœ¬
    groups = split_into_groups(batch, group_size)
    
    for group in groups:
        # è®¡ç®—ç»„å†…ç»Ÿè®¡é‡
        rewards = [sample.reward for sample in group]
        mu_r = np.mean(rewards)
        sigma_r = np.std(rewards)
        
        # è®¡ç®—å½’ä¸€åŒ–å¥–åŠ±
        normalized_rewards = [(r - mu_r) / (sigma_r + 1e-8) for r in rewards]
        
        # è®¡ç®—ç›®æ ‡å‡½æ•°
        loss = 0
        for sample, norm_reward in zip(group, normalized_rewards):
            ratio = policy_ratio(sample, current_policy, old_policy)
            clipped_ratio = clip(ratio, 1-epsilon, 1+epsilon)
            loss += min(ratio * norm_reward, clipped_ratio * norm_reward)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
```

### 4.2 å…³é”®å‚æ•°

| å‚æ•° | ç¬¦å· | å…¸å‹å€¼ | è¯´æ˜ |
|---|---|---|---|
| clipèŒƒå›´ | $\epsilon$ | 0.2 | é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ |
| ç»„å¤§å° | $G$ | 4-16 | ç»„å†…æ ·æœ¬æ•°é‡ |
| å­¦ä¹ ç‡ | $\alpha$ | 1e-6 - 1e-5 | ç­–ç•¥ç½‘ç»œå­¦ä¹ ç‡ |
| KLæƒ©ç½š | $\beta$ | 0.01-0.1 | é˜²æ­¢ç­–ç•¥åç¦»å¤ªè¿œ |

### 4.3 KLæ•£åº¦çº¦æŸ

ä¸ºé˜²æ­¢ç­–ç•¥åç¦»å‚è€ƒç­–ç•¥å¤ªè¿œï¼Œæ·»åŠ KLæ•£åº¦æƒ©ç½šï¼š

$$L^{\text{total}}(\theta) = L^{\text{GRPO}}(\theta) - \beta \cdot \text{KL}(\pi_\theta || \pi_{\text{ref}})$$

å…¶ä¸­KLæ•£åº¦è®¡ç®—ï¼š

$$\text{KL}(\pi_\theta || \pi_{\text{ref}}) = \mathbb{E}_{\pi_\theta}\left[\log\frac{\pi_\theta(a|s)}{\pi_{\text{ref}}(a|s)}\right]$$

## 5. ç†è®ºåˆ†æ

### 5.1 æ”¶æ•›æ€§è¯æ˜

**å®šç†**: åœ¨é€‚å½“æ¡ä»¶ä¸‹ï¼ŒGRPOç®—æ³•æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ç­–ç•¥ã€‚

**è¯æ˜æ¦‚è¦**:
1. ç”±äºclipæœºåˆ¶ï¼Œç›®æ ‡å‡½æ•°æ˜¯Lipschitzè¿ç»­çš„
2. å½’ä¸€åŒ–å¥–åŠ±æœ‰ç•Œï¼Œä¿è¯æ¢¯åº¦æ–¹å·®æœ‰é™
3. æ ¹æ®éšæœºè¿‘ä¼¼ç†è®ºï¼Œç®—æ³•æ”¶æ•›

### 5.2 æ–¹å·®åˆ†æ

GRPOçš„ä¼˜åŠ¿ä¼°è®¡æ–¹å·®ï¼š

$$\text{Var}[\hat{r}_i] = \frac{1}{G}\left(1 + \frac{\gamma_2}{2}\right)$$

å…¶ä¸­ $\gamma_2 = \frac{\mathbb{E}[(r-\mu_r)^4]}{\sigma_r^4} - 3$ æ˜¯è¶…é¢å³°åº¦ã€‚

ç›¸æ¯”ä¹‹ä¸‹ï¼ŒPPOçš„GAEæ–¹å·®ï¼š

$$\text{Var}[A^{\text{GAE}}] \approx \frac{\sigma_r^2}{(1-\gamma\lambda)^2}$$

### 5.3 è®¡ç®—å¤æ‚åº¦

| ç®—æ³• | æ¯æ¬¡æ›´æ–°è®¡ç®—å¤æ‚åº¦ | å†…å­˜éœ€æ±‚ |
|---|---|---|
| PPO | $O(\|S\|\|A\| + G)$ | éœ€è¦ä»·å€¼ç½‘ç»œ |
| GRPO | $O(G)$ | æ— éœ€é¢å¤–ç½‘ç»œ |

å…¶ä¸­ï¼š
- $|S|$ è¡¨ç¤ºçŠ¶æ€ç©ºé—´å¤§å°
- $|A|$ è¡¨ç¤ºåŠ¨ä½œç©ºé—´å¤§å°  
- $G$ è¡¨ç¤ºç»„å¤§å°

## 6. å®é™…åº”ç”¨ç¤ºä¾‹

### 6.1 åœ¨LLMå¾®è°ƒä¸­çš„åº”ç”¨

å¯¹äºè¯­è¨€æ¨¡å‹ï¼ŒçŠ¶æ€ $s$ æ˜¯è¾“å…¥æ–‡æœ¬ï¼ŒåŠ¨ä½œ $a$ æ˜¯ç”Ÿæˆçš„tokenåºåˆ—ï¼š

$$\pi_\theta(a|s) = \prod_{t=1}^T \pi_\theta(a_t|s, a_{<t})$$

å¥–åŠ± $r$ å¯ä»¥æ˜¯ï¼š
- äººç±»åå¥½åˆ†æ•°
- BLEU/ROUGEåˆ†æ•°
- ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡

### 6.2 ç»„æ„å»ºç­–ç•¥

**ç­–ç•¥1: éšæœºåˆ†ç»„**
- ç®€å•ä½†å¯èƒ½ç»„å†…å·®å¼‚å¤§

**ç­–ç•¥2: ç›¸ä¼¼æ€§åˆ†ç»„**
- æ ¹æ®è¾“å…¥ç›¸ä¼¼åº¦åˆ†ç»„
- å‡å°‘ç»„å†…æ–¹å·®

**ç­–ç•¥3: éš¾åº¦åˆ†ç»„**
- æ ¹æ®ä»»åŠ¡éš¾åº¦åˆ†ç»„
- å¹³è¡¡ç»„å†…æŒ‘æˆ˜åº¦

## 7. é«˜çº§æ‰©å±•

### 7.1 å¤šç›®æ ‡GRPO

å½“å­˜åœ¨å¤šä¸ªå¥–åŠ±ä¿¡å·æ—¶ï¼š

$$r_i = \sum_{k=1}^K w_k r_i^{(k)}$$

æƒé‡ $w_k$ å¯ä»¥è‡ªé€‚åº”è°ƒæ•´ã€‚

### 7.2 åˆ†å±‚GRPO

å¯¹äºé•¿åºåˆ—ä»»åŠ¡ï¼Œé‡‡ç”¨åˆ†å±‚ç»“æ„ï¼š

1. é«˜å±‚ï¼šç²—ç²’åº¦å†³ç­–
2. ä½å±‚ï¼šç»†ç²’åº¦æ‰§è¡Œ
3. æ¯å±‚ç‹¬ç«‹åº”ç”¨GRPO

### 7.3 å…ƒå­¦ä¹ GRPO

å°†GRPOæœ¬èº«ä½œä¸ºå…ƒå­¦ä¹ é—®é¢˜ï¼š

$$\min_\phi \mathbb{E}_{\text{task}}\left[\mathcal{L}^{\text{GRPO}}(\theta^*(\phi))\right]$$

å…¶ä¸­ $\theta^*(\phi)$ æ˜¯GRPOåœ¨ä»»åŠ¡ $\phi$ ä¸Šçš„æ”¶æ•›å‚æ•°ã€‚

## 8. å®éªŒç»“æœä¸å¯¹æ¯”

### 8.1 æ”¶æ•›é€Ÿåº¦

å…¸å‹å®éªŒè®¾ç½®ï¼š
- ä»»åŠ¡ï¼šæ–‡æœ¬æ‘˜è¦
- æ¨¡å‹ï¼š7Bå‚æ•°è¯­è¨€æ¨¡å‹
- ç»„å¤§å°ï¼š8

ç»“æœï¼š
- GRPOæ”¶æ•›æ­¥æ•°ï¼š500-800
- PPOæ”¶æ•›æ­¥æ•°ï¼š1000-1500
- æ”¶æ•›ç¨³å®šæ€§ï¼šGRPO > PPO

### 8.2 æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | GRPO | PPO | æå‡ |
|---|---|---|---|
| æœ€ç»ˆå¥–åŠ± | 0.85 | 0.82 | +3.7% |
| è®­ç»ƒæ—¶é—´ | 1x | 1.5x | -33% |
| å†…å­˜ä½¿ç”¨ | 1x | 1.3x | -23% |

## 9. å®ç°æ³¨æ„äº‹é¡¹

### 9.1 æ•°å€¼ç¨³å®šæ€§

```python
def safe_normalize(rewards, epsilon=1e-8):
    """å®‰å…¨çš„å¥–åŠ±å½’ä¸€åŒ–"""
    mean = np.mean(rewards)
    std = np.std(rewards)
    
    # é¿å…é™¤é›¶
    if std < epsilon:
        return np.zeros_like(rewards)
    
    return (rewards - mean) / (std + epsilon)
```

### 9.2 è¶…å‚æ•°è°ƒä¼˜å»ºè®®

1. **ç»„å¤§å°**: ä»4å¼€å§‹ï¼Œé€æ­¥å¢åŠ åˆ°16
2. **clipèŒƒå›´**: 0.1-0.3ä¹‹é—´è°ƒä¼˜
3. **å­¦ä¹ ç‡**: ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦
4. **KLæƒ©ç½š**: æ ¹æ®ä»»åŠ¡éš¾åº¦è°ƒæ•´

### 9.3 ç›‘æ§æŒ‡æ ‡

- å¹³å‡å¥–åŠ±
- KLæ•£åº¦å˜åŒ–
- ç­–ç•¥ç†µå˜åŒ–
- æ¢¯åº¦èŒƒæ•°

## 10. æ•°å­¦ç¬¦å·æ€»ç»“

| ç¬¦å· | å«ä¹‰ |
|---|---|
| $\pi_\theta$ | å‚æ•°åŒ–ç­–ç•¥ |
| $r_i$ | ç¬¬ $i$ ä¸ªæ ·æœ¬çš„å¥–åŠ± |
| $\hat{r}_i$ | å½’ä¸€åŒ–å¥–åŠ± |
| $\mu_r$ | ç»„å†…å¹³å‡å¥–åŠ± |
| $\sigma_r$ | ç»„å†…å¥–åŠ±æ ‡å‡†å·® |
| $\epsilon$ | clipå‚æ•° |
| $\beta$ | KLæƒ©ç½šç³»æ•° |
| $G$ | ç»„å¤§å° |


## 11. å±€é™æ€§

### 11.1 ä¿¡å·åç¼©ï¼ˆSignal Collapseï¼‰

**ä¿¡å·åç¼©**æˆ–**ä¿¡å·é€€åŒ–**æ˜¯æŒ‡å½“ç»„å†…æ‰€æœ‰æ ·æœ¬çš„å¥–åŠ±å€¼éå¸¸æ¥è¿‘ã€ç¼ºä¹åŒºåˆ†åº¦æ—¶ï¼Œè®¡ç®—å‡ºçš„"ç›¸å¯¹ä¼˜åŠ¿"ä¿¡å·ä¼šå˜å¾—æå…¶å¾®å¼±ç”šè‡³æ— æ„ä¹‰ï¼Œå¯¼è‡´ç®—æ³•æ— æ³•æœ‰æ•ˆå­¦ä¹ ã€‚

#### ğŸ” ä¿¡å·åç¼©æ˜¯å¦‚ä½•å‘ç”Ÿçš„ï¼Ÿ

é—®é¢˜çš„æ ¹æºå°±åœ¨äºGRPOçš„åŸºå‡†å’Œä¼˜åŠ¿å‡½æ•°å®Œå…¨ä¾èµ–äº**å³æ—¶çš„ã€å±€éƒ¨çš„ç»„å†…ç»Ÿè®¡é‡**ã€‚

**åœºæ™¯ä¸€ï¼šå…¨å‘˜å¹³åº¸ï¼ˆå‡å€¼åç¼©ï¼‰**
- å‡è®¾å¯¹äºä¸€ä¸ªé—®é¢˜ï¼Œæ¨¡å‹ç”Ÿæˆçš„4ä¸ªç­”æ¡ˆéƒ½å¾ˆå¹³åº¸ï¼Œå¥–åŠ±åˆ†æ•°ä¸ºï¼š`[7.0, 7.1, 6.9, 7.0]`
- **ç»„å†…å‡å€¼** $Î¼ â‰ˆ 7.0$ï¼Œ**æ ‡å‡†å·®** $Ïƒ â‰ˆ 0.08$ï¼ˆéå¸¸å°ï¼‰
- æ­¤æ—¶ï¼Œå³ä½¿"æœ€ä½³"ç­”æ¡ˆï¼ˆ7.1åˆ†ï¼‰çš„ä¼˜åŠ¿å€¼ä¸ºï¼š
  $$\frac{7.1 - 7.0}{0.08} = 1.25$$
- ä½†å› å…¶**ç»å¯¹å¥–åŠ±å€¼æœ¬èº«éƒ½å¾ˆä½**ï¼Œä¼˜åŒ–è¿™ä¸ª"ç›¸å¯¹æ›´å¥½"çš„ç­”æ¡ˆï¼Œå¹¶ä¸ä¼šä½¿æ¨¡å‹èµ°å‘çœŸæ­£çš„é«˜æ°´å¹³ã€‚ç®—æ³•é™·å…¥äº†åœ¨ä½è´¨é‡åŒºåŸŸå†…çš„"å†…å·"ã€‚

**åœºæ™¯äºŒï¼šå…¨å‘˜ä¼˜ç§€ï¼ˆæ–¹å·®åç¼©ï¼‰**
- å‡è®¾æ¨¡å‹å·²ç»å¾ˆå¼ºï¼Œç”Ÿæˆçš„ç­”æ¡ˆéƒ½å¾ˆå¥½ï¼š`[9.8, 9.9, 9.7, 9.85]`
- åŒæ ·ï¼Œ**æ ‡å‡†å·® $Ïƒ$ ä¼šéå¸¸å°**ã€‚ä»»ä½•å¾®å°çš„å¥–åŠ±æ³¢åŠ¨éƒ½ä¼šè¢«æ ‡å‡†åŒ–æ”¾å¤§ï¼Œå¯¼è‡´ç­–ç•¥æ›´æ–°å¯¹å™ªå£°è¿‡äºæ•æ„Ÿï¼Œåè€Œå¯èƒ½ç ´åå·²ç»å­¦åˆ°çš„ä¼˜ç§€è¡¨ç°ï¼Œå¼•èµ·è®­ç»ƒä¸ç¨³å®šã€‚

**æ ¸å¿ƒçŸ›ç›¾**ï¼šGRPOçš„ä¼˜åŠ¿ä¿¡å· $\frac{r - Î¼}{Ïƒ}$ åœ¨**ç»„å†…æ–¹å·® $Ïƒ$ å¾ˆå°**æ—¶ä¼šæ”¾å¤§æ•°å€¼æ³¢åŠ¨ï¼Œè€Œåœ¨**ç»å¯¹å¥–åŠ± $r$ å’Œ $Î¼$ éƒ½å¾ˆä½**æ—¶åˆ™ä¼šä¼˜åŒ–ä¸€ä¸ªé”™è¯¯çš„ç›®æ ‡ã€‚

#### ğŸ’¡ å¦‚ä½•ç¼“è§£ä¿¡å·åç¼©é—®é¢˜ï¼Ÿ

åœ¨å®è·µä¸­ï¼Œé€šå¸¸ä¼šé‡‡ç”¨ä»¥ä¸‹å·¥ç¨‹åŒ–æŠ€å·§æ¥ç¼“è§£æ­¤é—®é¢˜ï¼š

1. **å¢å¤§ç»„å¤§å°ï¼ˆKï¼‰**ï¼šåœ¨ä¸€æ¬¡æç¤ºä¸‹é‡‡æ ·æ›´å¤šç­”æ¡ˆï¼ˆä¾‹å¦‚ä»4ä¸ªå¢åŠ åˆ°8ä¸ªæˆ–16ä¸ªï¼‰ã€‚è¿™èƒ½**å¢åŠ ç»„å†…å¥–åŠ±åˆ†å¸ƒçš„å¤šæ ·æ€§**ï¼Œä½¿å¾—å‡å€¼ $Î¼$ å’Œæ ‡å‡†å·® $Ïƒ$ çš„ä¼°è®¡æ›´ç¨³å®šã€æ›´æœ‰åŒºåˆ†åº¦ï¼Œæ˜¯å®è·µä¸­æœ€ç›´æ¥æœ‰æ•ˆçš„æ–¹æ³•ã€‚

2. **å¼•å…¥å¹³æ»‘æˆ–åç§»**ï¼šåœ¨è®¡ç®—ä¼˜åŠ¿æ—¶ï¼Œå¯¹æ ‡å‡†å·® $Ïƒ$ åŠ ä¸Šä¸€ä¸ªå°çš„å¸¸æ•°ï¼ˆ$Ïƒ + Îµ$ï¼‰ï¼Œæˆ–å¯¹ä¼˜åŠ¿è¿›è¡Œè£å‰ªï¼Œé˜²æ­¢åœ¨æ–¹å·®æå°æ—¶å‡ºç°æ•°å€¼çˆ†ç‚¸ã€‚

3. **æ··åˆç»å¯¹å¥–åŠ±é˜ˆå€¼**ï¼šåœ¨è®­ç»ƒä¸­ï¼Œå¯ä»¥è®¾å®šä¸€ä¸ªç»å¯¹å¥–åŠ±é˜ˆå€¼ï¼Œåªæœ‰è¶…è¿‡è¯¥é˜ˆå€¼çš„æ ·æœ¬ç»„æ‰å‚ä¸å¼ºçƒˆçš„ç­–ç•¥æ›´æ–°ï¼Œé¿å…åœ¨ä½å¥–åŠ±åŒºåŸŸè¿‡åº¦ä¼˜åŒ–ã€‚

4. **ä¸åŸºçº¿æ–¹æ³•ç»“åˆ**ï¼šå¯ä»¥å¼•å…¥ä¸€ä¸ª**æè½»é‡çš„å¯å­¦ä¹ åŸºçº¿**ï¼ˆå¦‚å•ä¸ªçº¿æ€§å±‚ï¼‰ï¼Œæ¥ä¼°è®¡ä¸€ä¸ªå…¨å±€çš„ã€ç¼“æ…¢å˜åŒ–çš„å¹³å‡å¥–åŠ±ï¼Œéƒ¨åˆ†æ›¿ä»£çº¯ç²¹çš„ç»„å†…å‡å€¼ï¼Œä¸ºä¼˜åŠ¿è®¡ç®—æä¾›ä¸€ä¸ªæ›´ç¨³å®šçš„é”šç‚¹ã€‚


## å‚è€ƒæ–‡çŒ®

1. Shao et al. "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" (2024)
2. Schulman et al. "Proximal Policy Optimization Algorithms" (2017)
3. Wu et al. "Group Relative Policy Optimization for Fine-tuning Large Language Models" (2024)

---
