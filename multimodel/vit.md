# Vision Transformer (ViT) åœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„å¯¹é½

**æ ¸å¿ƒä¸»é¢˜**: ViT å¦‚ä½•é€šè¿‡ä½ç½®ç¼–ç å’Œè·¨æ¨¡æ€å¯¹é½å®ç°è§†è§‰å®šä½èƒ½åŠ›ã€‚

---

## ğŸ“š ViT åŸºç¡€å›é¡¾

### ViT æ¶æ„æ¦‚è¿°

Vision Transformer (ViT) æ˜¯å°† Transformer æ¶æ„åº”ç”¨äºè®¡ç®—æœºè§†è§‰çš„é‡Œç¨‹ç¢‘å¼å·¥ä½œï¼Œç”± Google åœ¨ 2020 å¹´æå‡ºã€‚

**æ ¸å¿ƒæ€æƒ³**: å°†å›¾åƒåˆ†å‰²æˆ patchesï¼ˆå›¾åƒå—ï¼‰ï¼Œå°†æ¯ä¸ª patch å½“ä½œ tokenï¼Œè¾“å…¥åˆ° Transformer ä¸­å¤„ç†ã€‚

```python
# ViT åŸºæœ¬æµç¨‹
å›¾åƒ (HÃ—WÃ—C) â†’ åˆ†å‰²ä¸º N ä¸ª patch (PÃ—PÃ—C) â†’ å±•å¹³ä¸ºåºåˆ— â†’ æ·»åŠ ä½ç½®ç¼–ç  â†’ Transformer â†’ è¾“å‡º
```

### å…³é”®ç»„ä»¶

1. **Patch Embedding**: å°†å›¾åƒåˆ†å‰²æˆå›ºå®šå¤§å°çš„ patches
2. **Position Encoding**: ä¸ºæ¯ä¸ª patch æ·»åŠ  2D ä½ç½®ä¿¡æ¯
3. **Multi-Head Self-Attention**: å­¦ä¹  patch é—´çš„å…³ç³»
4. **MLP Blocks**: éçº¿æ€§å˜æ¢

> ğŸ’¡ **ä¸ºä»€ä¹ˆ ViT æœ‰æ•ˆï¼Ÿ** Transformer çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å¤©ç„¶é€‚åˆå»ºæ¨¡ patch é—´çš„å…¨å±€å…³ç³»ï¼Œè€Œ CNN çš„å±€éƒ¨æ„Ÿå—é‡é™åˆ¶äº†å…¨å±€å»ºæ¨¡ã€‚

---

## ğŸ¯ ViT åœ¨å¤šæ¨¡æ€å¯¹é½ä¸­çš„ä½œç”¨

**å®Œå…¨æ­£ç¡®ï¼** ğŸ¯

ä½ è¿™å¥è¯â€”â€”

> "å…¶å®å°±æ˜¯å¯¹é½äº†ï¼Œå› ä¸ºè¾“å…¥äº† text å’Œå›¾ç‰‡ï¼ŒåŒæ—¶ SFT çš„æ—¶å€™åˆç»™å‡ºäº†åæ ‡ï¼Œæ„ä¸º text æ‰€æè¿°çš„å›¾ç‰‡ä½ç½®çš„è¯­ä¹‰çš„å¯¹é½"

â€”â€”**ç²¾å‡†æ¦‚æ‹¬äº†å½“å‰å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆå¦‚ Qwen3-VLï¼‰å®ç°"è§†è§‰å®šä½"èƒ½åŠ›çš„æ ¸å¿ƒæœºåˆ¶**ã€‚

æˆ‘ä»¬å¯ä»¥æŠŠå®ƒæ‹†è§£ä¸ºä¸‰ä¸ªå…³é”®ç¯èŠ‚ï¼Œæ­£å¥½å¯¹åº”ç°ä»£å¤šæ¨¡æ€æ¨¡å‹çš„è®­ç»ƒ pipelineï¼š

---

### âœ… 1. **è¾“å…¥ï¼šå›¾æ–‡å¯¹ï¼ˆImage + Textï¼‰**
- å›¾åƒ â†’ è¢« ViT ç¼–ç ä¸ºä¸€ç»„å¸¦ **2D ä½ç½®æ„ŸçŸ¥** çš„è§†è§‰ tokenï¼›
- æ–‡æœ¬ â†’ è¢« LLM tokenizer ç¼–ç ä¸ºè¯­è¨€ tokenï¼›
- ä¸¤è€…åœ¨åŒä¸€ä¸ª Transformer ä¸­æ‹¼æ¥ï¼Œè¿›è¡Œè·¨æ¨¡æ€äº¤äº’ã€‚

> æ­¤æ—¶æ¨¡å‹ **å…·å¤‡äº†"å›¾æ–‡è”åˆç†è§£"çš„å¯èƒ½æ€§**ï¼Œä½†è¿˜ä¸çŸ¥é“"å“ªé‡Œå¯¹åº”å“ªé‡Œ"ã€‚

#### ViT ç¼–ç ç»†èŠ‚

```python
# ViT ç¼–ç è¿‡ç¨‹
def vit_encode(image):
    # 1. Patch åˆ†å‰²: 224Ã—224 å›¾åƒ â†’ 16Ã—16 patches (æ¯ä¸ª 14Ã—14)
    patches = image.unfold(0, 16, 16).unfold(1, 16, 16)  # [N, C, H, W] â†’ [N, num_patches, patch_size]

    # 2. å±•å¹³ä¸ºåºåˆ—: [num_patches, patch_dim]
    patch_embeddings = patches.flatten(2).transpose(1, 2)

    # 3. æ·»åŠ ä½ç½®ç¼–ç : ä¸ºæ¯ä¸ª patch æ·»åŠ  2D ä½ç½®ä¿¡æ¯
    # ä½ç½®ç¼–ç è€ƒè™‘äº† patch çš„ (row, col) åæ ‡
    pos_embed = get_2d_position_embeddings(num_patches_h, num_patches_w)
    patch_embeddings += pos_embed

    # 4. Transformer å¤„ç†
    encoded_tokens = transformer_encoder(patch_embeddings)

    return encoded_tokens  # [num_patches, embed_dim]
```

> ğŸ“Œ **ä½ç½®ç¼–ç çš„å…³é”®**: æ¯ä¸ªè§†è§‰ token ä¸ä»…åŒ…å«è§†è§‰å†…å®¹ï¼Œè¿˜åµŒå…¥å…¶åœ¨åŸå›¾ä¸­çš„ 2D åæ ‡ä¿¡æ¯ã€‚

---

### âœ… 2. **ç›‘ç£ä¿¡å·ï¼šSFTï¼ˆç›‘ç£å¾®è°ƒï¼‰ä¸­çš„åæ ‡æ ‡æ³¨**
åœ¨ SFT é˜¶æ®µï¼Œæ¨¡å‹åœ¨å¤§é‡ **(image, instruction, ground-truth bbox)** ä¸‰å…ƒç»„ä¸Šè®­ç»ƒï¼Œä¾‹å¦‚ï¼š

```text
User: ç‚¹å‡»å›¾ä¸­â€œå¯¼å‡ºâ€æŒ‰é’®ã€‚
Model: <box>(x1, y1, x2, y2)</box>
```

è¿™äº› bbox æ ‡æ³¨æ¥è‡ªï¼š
- **RefCOCO / RefCOCO+ / RefCLEF**ï¼šæŒ‡ä»£è¡¨è¾¾ç†è§£ï¼ˆreferring expression comprehensionï¼‰ï¼›
- **DocVQA / InfographicsVQA**ï¼šæ–‡æ¡£/å›¾è¡¨é—®ç­”ï¼›
- **UIBench / RicoSCA**ï¼šUI ç†è§£æ•°æ®é›†ã€‚

> ğŸ“Œ **è¿™äº›åæ ‡å°±æ˜¯â€œå¯¹é½ä¿¡å·â€**ï¼šå®ƒæ˜ç¡®å‘Šè¯‰æ¨¡å‹â€”â€”  
> â€œå½“ä½ è¯´â€˜å¯¼å‡ºæŒ‰é’®â€™æ—¶ï¼Œå¯¹åº”çš„è§†è§‰åŒºåŸŸæ˜¯è¿™ä¸ªçŸ©å½¢â€ã€‚

---

### âœ… 3. **å¯¹é½çš„æœ¬è´¨ï¼šå­¦ä¹ â€œè¯­è¨€æè¿° â†” è§†è§‰åŒºåŸŸâ€çš„æ˜ å°„**
é€šè¿‡ SFTï¼Œæ¨¡å‹å­¦ä¼šï¼š
- å½“æ–‡æœ¬ä¸­å‡ºç° **ç©ºé—´å…³é”®è¯**ï¼ˆå¦‚â€œå·¦ä¸Šè§’â€â€œä¸‹æ–¹â€â€œçº¢è‰²çš„â€ï¼‰ï¼›
- ä¸”è§†è§‰ token ä¸­å­˜åœ¨ **åŒ¹é…è¯­ä¹‰**ï¼ˆå¦‚æŒ‰é’®ã€æ–‡å­—ã€å›¾æ ‡ï¼‰ï¼›
- **ä¸”è¿™äº›è§†è§‰ token çš„ 2D ä½ç½®ç¼–ç  (i,j) ä¸æè¿°ä¸€è‡´**ï¼›
- â†’ å°±ç”Ÿæˆå¯¹åº”çš„åæ ‡ã€‚

> è¿™ä¸æ˜¯å‡ ä½•æ¨ç†ï¼Œè€Œæ˜¯ **è·¨æ¨¡æ€è¯­ä¹‰-ç©ºé—´å¯¹é½çš„ç«¯åˆ°ç«¯æ‹Ÿåˆ**ã€‚

æ¨¡å‹å†…éƒ¨å¹¶æ²¡æœ‰â€œè®¡ç®—åæ ‡â€çš„æ¨¡å—ï¼Œè€Œæ˜¯ï¼š
> **è¯­è¨€æç¤º + è§†è§‰ tokenï¼ˆå«è¯­ä¹‰ + 2D ä½ç½®ï¼‰ â†’ ç”Ÿæˆåæ ‡æ–‡æœ¬**

è¿™æ­£æ˜¯ä½ æ‰€è¯´çš„ï¼šâ€œ**text æ‰€æè¿°çš„å›¾ç‰‡ä½ç½®çš„è¯­ä¹‰çš„å¯¹é½**â€ã€‚

---

### ğŸ” ç±»æ¯”ç†è§£

å¯ä»¥æŠŠè¿™ä¸ªè¿‡ç¨‹ç±»æ¯”ä¸ºæ•™å°å­©è®¤å›¾ï¼š

- ä½ æŒ‡ç€ä¸€å¼  UI æˆªå›¾è¯´ï¼šâ€œè¿™æ˜¯â€˜ä¿å­˜â€™æŒ‰é’®â€ï¼ˆå¹¶ç”¨æ‰‹æŒ‡åœˆå‡ºåŒºåŸŸï¼‰ï¼›
- é‡å¤ thousands æ¬¡åï¼Œå°å­©çœ‹åˆ°æ–°å›¾ï¼Œå¬åˆ°â€œä¿å­˜æŒ‰é’®â€ï¼Œå°±èƒ½å¤§è‡´æŒ‡å‡ºä½ç½®ï¼›
- ä½†ä»–**å¹¶ä¸çŸ¥é“åƒç´ åæ ‡**ï¼Œåªæ˜¯**æŠŠè¯­è¨€ã€è§†è§‰ã€ç©ºé—´ä½ç½®å…³è”èµ·æ¥äº†**ã€‚

Qwen3-VL å°±æ˜¯è¿™ä¸ªâ€œå°å­©â€ï¼Œè€Œ bbox æ ‡æ³¨å°±æ˜¯ä½ çš„â€œæ‰‹æŒ‡åœˆå‡ºåŒºåŸŸâ€ã€‚

---

### âš ï¸ ä½†è¦æ³¨æ„ï¼šè¿™ç§å¯¹é½æœ‰å±€é™

| ä¼˜åŠ¿ | å±€é™ |
|------|------|
| âœ… é€šç”¨æ€§å¼ºï¼šæ”¯æŒä»»æ„è¯­è¨€æè¿° | âŒ ä¾èµ–è®­ç»ƒæ•°æ®è¦†ç›–ï¼ˆæ²¡è§è¿‡çš„ UI é£æ ¼å¯èƒ½å¤±æ•ˆï¼‰ |
| âœ… æ— éœ€é¢å¤–æ£€æµ‹æ¨¡å‹ | âŒ å®šä½ç²¾åº¦å—é™äº patch ç²’åº¦ï¼ˆ~14pxï¼‰ |
| âœ… æ”¯æŒå¤æ‚è¯­ä¹‰ï¼ˆâ€œç°è‰²çš„ã€ä¸å¯ç‚¹å‡»çš„è¾“å…¥æ¡†â€ï¼‰ | âŒ æ— æ³•å¤„ç†æ— æ ‡æ³¨æ¦‚å¿µï¼ˆå¦‚æ–°å›¾æ ‡ï¼‰ |
| âœ… ç«¯åˆ°ç«¯ï¼Œç®€å•ç›´æ¥ | âŒ åæ ‡æ˜¯â€œç”Ÿæˆçš„æ–‡æœ¬â€ï¼Œä¸æ˜¯â€œè®¡ç®—çš„å‡ ä½•é‡â€ |

---

### ğŸ’¡ å¯¹ä½ çš„æ„ä¹‰ï¼ˆmacOS å¼€å‘è€…ï¼‰

ä½ å·²ç»ç†è§£äº†æœ¬è´¨ï¼š**Qwen3-VL çš„å®šä½èƒ½åŠ› = SFT é˜¶æ®µå­¦æ¥çš„å›¾æ–‡-åæ ‡å¯¹é½**ã€‚

å› æ­¤ï¼Œåœ¨å®è·µä¸­ä½ å¯ä»¥ï¼š
1. **ç”¨è‡ªç„¶è¯­è¨€æè¿° UI å…ƒç´ **ï¼ˆè¶Šå…·ä½“è¶Šå¥½ï¼Œå¦‚â€œå³ä¸Šè§’ç°è‰²çš„å…³é—­æŒ‰é’®â€ï¼‰ï¼›
2. **ä¿¡ä»»æ¨¡å‹åœ¨å¸¸è§ UI ä¸Šçš„è¾“å‡º**ï¼ˆå›  UIBench ç­‰æ•°æ®é›†è¦†ç›–äº†å¤§é‡æŒ‰é’®/è¾“å…¥æ¡†ï¼‰ï¼›
3. **å¯¹å…³é”®æ“ä½œåŠ éªŒè¯**ï¼ˆç”¨ AXUIElement æ£€æŸ¥æ˜¯å¦çœŸç‚¹åˆ°äº†ï¼‰ï¼›
4. **é¿å…æè¿°æ¨¡ç³Šæˆ–è®­ç»ƒæœªè¦†ç›–çš„å…ƒç´ **ï¼ˆå¦‚è‡ªå®šä¹‰ç»˜å›¾æ§ä»¶ï¼‰ã€‚

---

---

## ğŸ”§ å…¶ä»–å¯¹é½æŠ€æœ¯

### å¯¹æ¯”å­¦ä¹  (Contrastive Learning)

é™¤äº†åæ ‡æ ‡æ³¨ï¼Œæ—©æœŸå¤šæ¨¡æ€å¯¹é½å¸¸ç”¨å¯¹æ¯”å­¦ä¹ ï¼š

```python
# CLIP é£æ ¼çš„å¯¹æ¯”æŸå¤±
def clip_loss(image_features, text_features, temperature=0.07):
    # å½’ä¸€åŒ–ç‰¹å¾
    image_features = F.normalize(image_features, dim=-1)
    text_features = F.normalize(text_features, dim=-1)

    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity = image_features @ text_features.T / temperature

    # å¯¹æ¯”æŸå¤±ï¼šåŒ¹é…çš„å›¾æ–‡å¯¹ç›¸ä¼¼åº¦é«˜ï¼Œä¸åŒ¹é…çš„ä½
    labels = torch.arange(len(similarity)).to(device)
    loss_i2t = F.cross_entropy(similarity, labels)
    loss_t2i = F.cross_entropy(similarity.T, labels)

    return (loss_i2t + loss_t2i) / 2
```

> ğŸ“Œ **å¯¹æ¯”å­¦ä¹ ä¼˜åŠ¿**: æ— éœ€ç²¾ç¡®æ ‡æ³¨ï¼Œåªéœ€è¦"åŒ¹é…/ä¸åŒ¹é…"ä¿¡å·ã€‚

### è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶

åœ¨ç»Ÿä¸€ Transformer ä¸­ï¼Œè§†è§‰å’Œæ–‡æœ¬ token é€šè¿‡æ³¨æ„åŠ›äº¤äº’ï¼š

```python
# è·¨æ¨¡æ€æ³¨æ„åŠ›è®¡ç®—
def cross_modal_attention(visual_tokens, text_tokens):
    # æ‹¼æ¥è§†è§‰å’Œæ–‡æœ¬ token
    all_tokens = torch.cat([visual_tokens, text_tokens], dim=1)

    # è‡ªæ³¨æ„åŠ›ï¼šè§†è§‰ token å¯ä»¥ attend åˆ°æ–‡æœ¬ tokenï¼Œåä¹‹äº¦ç„¶
    attended_tokens = self_attention(all_tokens)

    # åˆ†ç¦»å›è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤º
    visual_output = attended_tokens[:, :len(visual_tokens)]
    text_output = attended_tokens[:, len(visual_tokens):]

    return visual_output, text_output
```

---

## ğŸ’» å®ç°ä¸è®­ç»ƒ

### LoRA å¾®è°ƒé…ç½®

å¯¹äº Qwen3-VL è¿™æ ·çš„æ¨¡å‹ï¼Œè§†è§‰æ¨¡å—çš„ LoRA é…ç½®ï¼š

```python
from peft import LoraConfig

# è§†è§‰æ¨¡å— LoRA
visual_lora_config = LoraConfig(
    r=8,  # è§†è§‰æ¨¡å—ç”¨è¾ƒå°ç§©
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=[
        "visual.blocks.*.attn.qkv",    # è§†è§‰æ³¨æ„åŠ›
        "visual.blocks.*.mlp.fc1",     # è§†è§‰ MLP
        "visual.blocks.*.mlp.fc2"
    ]
)
```

### è®­ç»ƒæ•°æ®å‡†å¤‡

```json
{
  "instruction": "ç‚¹å‡»ç™»å½•æŒ‰é’®",
  "screenshot": "login_page.png",
  "action": {
    "type": "click",
    "bbox": [200, 150, 300, 200],
    "description": "è“è‰²çš„ç™»å½•æŒ‰é’®"
  }
}
```

### æ¨ç†ä½¿ç”¨

```python
def predict_coordinates(image_path, instruction):
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image_path},
                {"type": "text", "text": instruction}
            ]
        }
    ]

    # æ¨¡å‹ç”Ÿæˆåæ ‡
    response = model.generate(messages)
    # è§£æ <box>(x1,y1,x2,y2)</box> æ ¼å¼

    return parse_bbox(response)
```

---

## ğŸ“Š æ€§èƒ½ä¸å±€é™

### å®šä½ç²¾åº¦åˆ†æ

| æ–¹æ³• | ç²¾åº¦ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| ViT + SFT | ~14px (patch å¤§å°) | UI è‡ªåŠ¨åŒ– |
| DETR | ~1-5px | ç²¾ç¡®æ£€æµ‹ |
| CNN + Anchor | ~2-10px | ä¼ ç»Ÿæ£€æµ‹ |

### å¸¸è§é—®é¢˜

1. **ä½ç½®åç§»**: Patch ç²’åº¦å¯¼è‡´çš„å®šä½ä¸ç²¾ç¡®
2. **é®æŒ¡å¤„ç†**: æ¨¡å‹å¯¹éƒ¨åˆ†å¯è§å…ƒç´ å®šä½èƒ½åŠ›å¼±
3. **åŠ¨æ€å…ƒç´ **: æ»šåŠ¨é¡µé¢æˆ–åŠ¨ç”»å…ƒç´ éš¾ä»¥å®šä½
4. **åˆ†è¾¨ç‡ä¾èµ–**: é«˜åˆ†è¾¨ç‡å›¾åƒéœ€è¦æ›´å¤§ patch æ•°é‡

---

## ğŸ”— ç›¸å…³èµ„æº

### è®ºæ–‡
- [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929) - ViT åŸè®ºæ–‡
- [CLIP: Learning Transferable Visual Models](https://arxiv.org/abs/2103.00020) - å¯¹æ¯”å­¦ä¹ å¯¹é½
- [LLaVA: Large Language and Vision Assistant](https://arxiv.org/abs/2304.08485) - å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒ

### æ•°æ®é›†
- **RefCOCO/RefCOCO+**: æŒ‡ä»£è¡¨è¾¾ç†è§£
- **UIBench**: UI å…ƒç´ æ ‡æ³¨
- **DocVQA**: æ–‡æ¡£é—®ç­”

### å·¥å…·åº“
- **transformers**: Hugging Face ViT å®ç°
- **timm**: PyTorch Image Models (ViT å˜ä½“)
- **OpenCLIP**: å¼€æº CLIP å®ç°

---

### âœ… æœ€ç»ˆæ€»ç»“

> **æ˜¯çš„ï¼ŒQwen3-VL çš„è§†è§‰å®šä½èƒ½åŠ›æœ¬è´¨ä¸Šæ˜¯é€šè¿‡ SFT é˜¶æ®µçš„åæ ‡æ ‡æ³¨ï¼Œå®ç°äº†"æ–‡æœ¬è¯­ä¹‰"ä¸"å›¾åƒåŒºåŸŸï¼ˆé€šè¿‡ patch ä½ç½®ç¼–ç è¡¨ç¤ºï¼‰"çš„ç«¯åˆ°ç«¯å¯¹é½ã€‚æ¨¡å‹å¹¶ä¸è®¡ç®—åæ ‡ï¼Œè€Œæ˜¯å­¦ä¼šåœ¨æ­£ç¡®è¯­å¢ƒä¸‹"è¯´å‡º"æ­£ç¡®çš„åæ ‡æ–‡æœ¬ã€‚**



---

## ğŸš€ æ‰©å±•é˜…è¯»

- [ViT è¯¦è§£](docs/cv/vit.md) - æ›´æ·±å…¥çš„ ViT æŠ€æœ¯ç»†èŠ‚
- [å¤šæ¨¡æ€è®­ç»ƒ](docs/model-train/qwen3-vl-sft.md) - å®Œæ•´è®­ç»ƒæŒ‡å—
- [è®¡ç®—æœºä½¿ç”¨ä»£ç†](docs/model-train/qwen3-vl-computer-use.md) - å®é™…åº”ç”¨æ¡ˆä¾‹