模型训练的标准步骤通常包括三个阶段：预训练（Pre-training）、SFT（监督微调）和 RLHF（基于人类反馈的强化学习）。

## 预训练阶段（Pre-training）

预训练是模型训练的基础阶段，主要特点包括：

- **数据规模**：使用海量无标注数据（数万亿token），涵盖网页、书籍、代码、科学文献等
- **训练目标**：自回归语言建模，预测下一个token
- **计算资源**：需要数千甚至上万张GPU，训练周期数月
- **关键考量**：
  - 数据质量过滤（去重、过滤低质量内容）
  - 数据配比（不同领域数据的混合比例）
  - 分词器（Tokenizer）选择与词表大小
  - 学习率调度策略
  - 训练稳定性（梯度裁剪、权重衰减等）

国产大模型的预训练特点：
- 中文数据占比通常较高
- 早期阶段使用更多中文语料，后期逐步增加英文比例
- 部分模型采用知识蒸馏策略，从已有强模型补充数据

## SFT阶段（监督微调）

SFT是将预训练模型适配到特定任务的关键阶段：

- **训练目标**：在标注数据上继续训练，学习指令遵循能力
- **数据特点**：
  - 格式通常为对话式（user-assistant交互）
  - 包含多样化的任务类型（问答、写作、编程、数学推理等）
  - 数据质量直接影响模型最终表现
- **训练策略**：
  - 学习率通常为预训练阶段的1/10~1/100
  - 训练轮数（epoch）较少，通常1~3轮
  - 混合不同来源的数据，保持分布平衡

### 国产模型SFT数据构建特点

**DeepSeek R1**：
- 采用冷启动方式，使用数千条高质量CoT数据
- 特殊结构化格式：`|special_token|<reasoning_process>|special_token|<summary>`
- 数据强调推理链的完整性和准确性

**Qwen**：
- 标准对话格式，使用`<|im_start|>`标记
- 数据经过两阶段严格过滤
- 强调数据的通用性和多样性

**GLM-4.5**：
- 标准对话格式
- 使用扩展CoT的小规模SFT数据
- 注重长文本理解和生成能力

**MiniMax**：
- 标准对话格式
- 多领域覆盖，数学和代码数据占比60%
- 强调专业领域能力

**Kimi 1.5**：
- 标准对话格式
- 包含规划、评估、反思和探索四种核心思考节点
- 强调模型的思考深度和自我纠错能力

## RLHF阶段（基于人类反馈的强化学习）

RLHF通过人类偏好信号进一步优化模型：

- **核心流程**：
  1. 训练奖励模型（Reward Model）：学习人类对不同输出的偏好排序
  2. 使用强化学习优化：让策略模型生成更高奖励的输出
- **主要算法**：
  - PPO（Proximal Policy Optimization）：OpenAI早期使用的主流方法
  - DPO（Direct Preference Optimization）：直接优化偏好，无需单独奖励模型
  - GRPO（Group Relative Policy Optimization）：DeepSeek R1采用的算法，完全摒弃价值网络

### 国产模型RLHF特点

**DeepSeek R1**：
- 采用GRPO算法，完全摒弃价值网络
- 多轮训练迭代优化模型表现
- 在推理任务上表现尤为突出

**Grok系列**：
- 大规模强化学习训练
- 训练量达到前代模型的10-100倍
- 强调实时信息理解和交互能力

## 训练数据工程

数据工程是模型训练成功的关键因素：

- **数据收集**：多源数据采集（网页、书籍、合成数据等）
- **数据清洗**：去重、过滤低质量内容、去除有害信息
- **数据标注**：高质量SFT和RM数据的构建
- **数据配比**：不同领域、不同任务数据的混合比例优化
- **合成数据**：使用强模型生成高质量训练数据

## 训练基础设施

- **硬件配置**：NVIDIA GPU集群（A100、H100等）
- **分布式训练**：数据并行、张量并行、流水线并行
- **通信优化**：InfiniBand高速网络、NVLink连接
- **容错机制**：检查点保存、故障恢复
- **资源调度**：弹性训练、动态资源分配

## 模型评估与验证

- **自动化评估**：标准 benchmark（PPL、MMLU、HellaSwag等）
- **人工评估**：真实用户体验测试
- **红队测试**：安全性和鲁棒性测试
- **消融实验**：验证关键设计决策的影响

## 国产模型训练流程总结

| 模型 | SFT特点 | RLHF特点 |
|------|---------|----------|
| DeepSeek R1 | 冷启动+结构化CoT | GRPO算法 |
| Qwen | 两阶段过滤 | 迭代优化 |
| GLM-4.5 | 扩展CoT | 标准RLHF |
| MiniMax | 数学代码为主 | 多轮迭代 |
| Kimi 1.5 | 四种思考节点 | 强化思考能力 |

DeepSeek R1的R1-Zero实验证明：跳过SFT直接使用RL激发模型推理能力是可行的，但存在语言混合、可读性差等问题。最终版本仍采用"冷启动SFT + RL"混合策略。
