# 大模型推理框架概述

当前主流的大模型推理框架主要包括 **vLLM、TensorRT-LLM、SGLang、Ollama、LMDeploy** 等，这些框架在性能、易用性和适用场景上各有侧重。

## 框架对比

| 框架 | 核心技术创新 | 适用场景 |
|------|--------------|----------|
| **vLLM** | PagedAttention（操作系统级内存管理） | 企业级高并发应用 |
| **TensorRT-LLM** | TensorRT深度优化（FP8/INT4量化、内核融合） | 对延迟要求极高的场景 |
| **SGLang** | RadixAttention（基数树KV缓存复用） | 多轮对话、结构化输出 |
| **Ollama** | 轻量级本地推理（Go + llama.cpp） | 个人开发、本地部署 |
| **LMDeploy** | 国产GPU优化（Turbomind引擎、W4A16量化） | 国产硬件、视觉语言任务 |

---

## vLLM

- GitHub: https://github.com/vllm-project/vllm

### 技术突破

**PagedAttention** 是 vLLM 的核心技术，借鉴操作系统内存分页机制管理 KV Cache：

- 传统方案显存利用率仅约 60%，vLLM 提升至 **95% 以上**
- 支持 **Continuous Batching**（连续批处理），而非传统的 static batching
- 在高并发场景下保持极低的 **TTFT**（首字出词时间）

### 性能指标

- 吞吐量比 HF 高 **14-24 倍**，比 TGI 高 **2.2-2.5 倍**
- Llama3.1-170B-FP8 单 H100：TTFT 仅 **123ms**，吞吐量可达 **15k tokens/s**
- 业界案例：Chatbot Arena、Vicuna 大模型的服务后端

### 注意

同样的模型、参数和 prompt 条件下，vLLM 推理和 Huggingface 推理结果可能不一致。

---

## TensorRT-LLM

### 技术突破

- 基于 **TensorRT 深度优化**，支持 **FP8/INT4 量化**
- 通过预编译优化生成高度精炼的 **TensorRT 引擎文件**
- **内核级优化** Transformer 核心组件（注意力、前馈网络等）
- 在 NVIDIA GPU 上实现 **微秒级延迟** 的极致性能

### 适用场景

- 实时客服系统
- 金融高频交易
- 需要快速响应的 API 服务

**局限**：仅限 NVIDIA CUDA 平台，跨平台部署存在局限

---

## SGLang

### 技术突破

**RadixAttention** 技术：

- 通过 **基数树（Radix Tree）** 对 KV 缓存的公共前缀进行高效复用
- 结合 **LRU 驱逐策略** 显著提升缓存利用率
- 支持 **结构化输出**，可直接生成符合规范的 JSON、XML 格式

### 性能指标

在 Llama-7B 模型上执行多轮对话任务时，吞吐量较 vLLM 提升达 **5 倍**。

### 适用场景

- 高并发、低延迟系统
- 大规模并行请求处理
- 多轮对话场景
- 结构化查询场景

---

## Ollama

- GitHub: https://github.com/ollama/ollama

### 技术突破

- **轻量级本地推理平台**，基于 **Go 语言** 封装
- 集成 **llama.cpp** 推理引擎
- 支持 **跨平台运行**（macOS、Windows、Linux）
- **完全离线运行**，保障数据安全
- **低硬件门槛**，易于部署

### 性能指标

- **冷启动时间**：仅 **12 秒**左右

### 适用场景

- 个人开发者
- 教育演示、原型验证
- 个人知识库
- 本地隐私要求高的场景

---

## LMDeploy

- GitHub: https://github.com/InternLM/lmdeploy

### 技术突破

上海人工智能实验室开发的部署工具箱：

- 集成 **Turbomind 异步推理引擎**，延迟 ≤ **50ms**
- **国产 GPU 深度优化**（昇腾 NPU 等）
- 支持 **W4A16 量化**（权重 INT4 激活 FP16），显著压缩显存占用
- **KV Cache INT8 量化**，进一步降低显存消耗
- **多模态融合**，支持视觉语言混合任务

### 性能指标

- TurboMind 的 output token throughput 超过 **2000 token/s**
- 比 huggingface transformers 提升 **2.3 倍**
- 比 vLLM 在 request throughput 上高 **30%**

### 适用场景

- 国产硬件环境部署
- 工业质检报告
- 金融实时风控
- 视觉语言混合任务

---

## 选型建议

| 需求场景 | 推荐框架 | 核心优势 |
|----------|----------|----------|
| 企业级高并发服务 | vLLM | PagedAttention、显存利用率高 |
| 对延迟要求极高 | TensorRT-LLM | 微秒级延迟、内核级优化 |
| 多轮对话/结构化输出 | SGLang | RadixAttention、缓存复用率5倍提升 |
| 个人开发/本地部署 | Ollama | 轻量级、12秒冷启动、跨平台 |
| 国产硬件环境 | LMDeploy | 昇腾NPU优化、多模态支持 |

### 选型关键因素

1. **硬件资源**：GPU/CPU/NPU 类型和数量
2. **性能要求**：延迟敏感型（TTFT）vs 吞吐量优先
3. **部署复杂度**：从简单（Ollama）到复杂（分布式部署）
4. **生态兼容性**：与现有技术栈的集成难度
5. **团队技术栈**：对框架的熟悉程度

### 建议

- **企业级应用**：优先 vLLM 或 TensorRT-LLM
- **个人开发/快速验证**：Ollama（启动快、易上手）
- **国产化场景**：LMDeploy（昇腾NPU原生支持）
- **多轮对话密集型**：SGLang（RadixAttention优势明显）
